{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVgOlzfMdiHe"
   },
   "source": [
    "<h1 style=\"font-size: 32px; font-weight: bold;\">Semantic Segmentation Competition (30%)</h1>\n",
    "\n",
    "For this competition, we will use a small autonomous driving dataset. The dataset contains 150 training images and 50 testing images.\n",
    "\n",
    "**We provide baseline code that includes the following features:**\n",
    "\n",
    "*    Loading the dataset using PyTorch.\n",
    "*    Defining a simple convolutional neural network for semantic segmentation.\n",
    "*    How to use existing loss function for the model learning.\n",
    "*    Train the network on the training data.\n",
    "*    Test the trained network on the testing data.\n",
    "<br/><br/>\n",
    "\n",
    "**The following changes could be considered:**\n",
    "\n",
    "1. Data augmentation\n",
    "2. Change of advanced training parameters: Learning Rate, Optimizer, Batch-size, and Drop-out.\n",
    "3. Architectural changes: Batch Normalization, Residual layers, etc.\n",
    "4. Use of a new loss function.\n",
    "\n",
    "Your code should be modified from the provided baseline. A pdf report of a maximum of two pages is required to explain the changes you made from the baseline, why you chose those changes, and the improvements they achieved.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "**Marking Rules:**\n",
    "\n",
    "We will mark the competition based on the final test accuracy on testing images and your report.\n",
    "\n",
    "Final mark (out of 50) = accuracy mark + efficiency mark + report mark\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "***Accuracy Mark 10:***\n",
    "\n",
    "We will rank all the submission results based on their test accuracy. Zero improvement over the baseline yields 0 marks. Maximum improvement over the baseline will yield 10 marks. There will be a sliding scale applied in between.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "***Efficiency Mark 10:***\n",
    "\n",
    "Efficiency considers not only the accuracy, but the computational cost of running the model (flops: https://en.wikipedia.org/wiki/FLOPS). Efficiency for our purposes is defined to be the ratio of accuracy (in %) to Gflops. Please report the computational cost for your final model and include the efficiency calculation in your report. Maximum improvement over the baseline will yield 10 marks. Zero improvement over the baseline yields zero marks, with a sliding scale in between.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "***Report mark 30:***\n",
    "\n",
    "**Your report should comprise:**\n",
    "\n",
    "1. An introduction showing your understanding of the task and of the baseline model: [10 marks]\n",
    "\n",
    "2. A description of how you have modified aspects of the system to improve performance. [10 marks]\n",
    "\n",
    "> A recommended way to present a summary of this is via an \"ablation study\" table, eg:\n",
    "\n",
    "> |Method1|Method2|Method3|Accuracy|\n",
    "> |---|---|---|---|\n",
    "> |N|N|N|60%|\n",
    "> |Y|N|N|65%|\n",
    "> |Y|Y|N|77%|\n",
    "> |Y|Y|Y|82%|\n",
    "\n",
    "3. Explanation of the methods for reducing the computational cost and/or improve the trade-off between accuracy and cost: [5 marks]\n",
    "\n",
    "4. Limitations/Conclusions: [5 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0dtUDczT_fB"
   },
   "source": [
    "# **1. Download Data & Set Configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747833719681,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "XCPZsWI_9s-Y"
   },
   "outputs": [],
   "source": [
    "##################################################################################################################################\n",
    "### Subject: Computer Vision\n",
    "### Year: 2025\n",
    "### Student Name: Max Busato, Liam Hennig\n",
    "### Student ID: a1851532, ?\n",
    "### Comptetion Name: Semantic Segmentation Competition\n",
    "### Final Results:\n",
    "### ACC:         GFLOPs:\n",
    "##################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747833719692,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "xHPzdgeP67Xu"
   },
   "source": [
    "## **Download the Dataset**\n",
    "\n",
    "Dowanload & Unzip the Dataset.\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Set Configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8909,
     "status": "ok",
     "timestamp": 1747833728595,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "106OzI7yUPON",
    "outputId": "84e98f0a-f948-4a7b-cd0d-aaa89e5d5570"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/busatmd021/projects/2025/COMPUTER-VISION/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ Standard Library ------------------------\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# ---------------------- External Dependencies ---------------------\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# --------------------------- PyTorch Core -------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ------------------------ PyTorch Ecosystem -----------------------\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as tf\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchinfo import summary\n",
    "\n",
    "# ------------------------ Data Augmentation -----------------------\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "#-------------------------------- Data path -----------------------------\n",
    "# Use your data path to replace the following path if you use Google drive.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Dataset Path. Ensure that the File Path Corresponds to the Path you Have Here. It is Expected that you Unzip the Data Folders before Running the Notebook.\n",
    "dataFolder = './Data/seg_data'\n",
    "\n",
    "# To access Google Colab GPU; Go To: Edit >>> Notebook Settings >>> Hardware Accelarator: Select GPU.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## **Edit Configurations to Tune Model Performance**\n",
    "\n",
    "These settings should be altered as part of the model's analysis. You could consider changing the training hyperparameters: Learning Rate, Optimiser, Batch-size, Number of Max Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! The device is CPU NOT GPU! Please avoid using CPU for training\n"
     ]
    }
   ],
   "source": [
    "# Editable Configuration Settings\n",
    "learning_rate = 1e-4 # Using a Learning Rate Scheduler\n",
    "width = 864\n",
    "height = 256 \n",
    "batchSize = 4 # Can Be Adjusted\n",
    "epochs = 200  # Can Be Adjusted\n",
    "\n",
    "# Data Check\n",
    "if not os.path.exists(dataFolder):\n",
    "  print('Data Path Error! Please check your data path')\n",
    "\n",
    "# CUDA Check\n",
    "if not torch.cuda.is_available():\n",
    "  print('WARNING! The device is CPU NOT GPU! Please avoid using CPU for training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iM0C-LdtUThm"
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "# **Define a Dataloader to Load Data**\n",
    "\n",
    "Add in better more systematic data transformations here. Data augmentation can be used (for example flip, resize). You should consider additional/better data augmentation, especially since there are so few training images for the model to learn from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1747833728629,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "_5dna2XkVHIq",
    "outputId": "ef7355a5-5c76-4bd4-8188-eb783007b805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded info for 150 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/busatmd021/projects/2025/COMPUTER-VISION/.venv/lib/python3.12/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# The Class to Load Images & Labels\n",
    "class ExpDataSet(Dataset):\n",
    "    def __init__(self, dataFolder):\n",
    "        # Define the Paths to the Training Image & Label Directories\n",
    "        self.image_dir = os.path.join(dataFolder, \"training/image\")\n",
    "        self.label_dir = os.path.join(dataFolder, \"training/label\")\n",
    "\n",
    "        # List & Sort All Image & Label Filenames to Ensure Correct Alignment\n",
    "        self.image_path = sorted(os.listdir(self.image_dir))\n",
    "        self.label_path = sorted(os.listdir(self.label_dir))\n",
    "\n",
    "        # Print the Number of Samples Loaded\n",
    "        print(f'Loaded info for {len(self.image_path)} images')\n",
    "\n",
    "        # Ensure that the Number of Images is Exactly What We Expect (sanity check)\n",
    "        assert len(self.image_path) == 150\n",
    "\n",
    "        # Loop Through Each Index To:\n",
    "            # Check that Image & Label Filenames Match\n",
    "            # Store Full Paths (Including Directory) for Loading Later\n",
    "        for idx in range(len(self.image_path)):\n",
    "            assert self.image_path[idx] == self.label_path[idx], f\"Image and label filenames do not match at index {idx}\"\n",
    "            self.image_path[idx] = os.path.join(self.image_dir, self.image_path[idx])\n",
    "            self.label_path[idx] = os.path.join(self.label_dir, self.label_path[idx])\n",
    "            \n",
    "            \n",
    "        # -------------------------------- Transformation Functions --------------------------------\n",
    "        self.transform = A.Compose([\n",
    "            # Resize to Fixed Size\n",
    "            A.Resize(height, width, interpolation=cv2.INTER_NEAREST),\n",
    "\n",
    "            # Geometric Augmentations\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.1,\n",
    "                scale_limit=0.1,\n",
    "                rotate_limit=20,\n",
    "                border_mode=cv2.BORDER_CONSTANT,  # Use Constant Padding\n",
    "                p=0.4\n",
    "            ),\n",
    "\n",
    "            # Photometric Augmentations (Only On Images, Won't Affect Masks)\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.ColorJitter(p=0.3),\n",
    "            A.HueSaturationValue(p=0.3),\n",
    "\n",
    "            # Blurring/Noise\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(p=0.2), \n",
    "                A.GaussianBlur(p=0.2), \n",
    "                A.GaussNoise(p=0.2)\n",
    "            ], p=0.2),\n",
    "\n",
    "            # Normalisation\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                        std=(0.229, 0.224, 0.225)),\n",
    "\n",
    "            # Convert to PyTorch Tensor\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load Image & Label Using OpenCV\n",
    "        img = cv2.imread(self.image_path[idx])[:, :, ::-1]  # Convert BGR to RGB\n",
    "        label = cv2.imread(self.label_path[idx], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Apply Albumentations (With Nearest-Neighbour Mask Handling)\n",
    "        augmented = self.transform(image=img, mask=label)\n",
    "        img = augmented['image']           # Tensor: (3, H, W)\n",
    "        label = augmented['mask'].long()   # Tensor: (H, W), type: long\n",
    "\n",
    "        # Return the Image & Mask\n",
    "        return img, label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "\n",
    "# Get the Predefined Dataloader\n",
    "exp_data = ExpDataSet(dataFolder)\n",
    "train_loader = DataLoader(exp_data, batch_size=batchSize, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwfMTxpbVVSt"
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "# **Define a Convolutional Neural Network**\n",
    "\n",
    "A new network should be used. Many architectural changes can be implemented, such as, Batch Normalisation, More layers, Residual layers, anything else you deem fit. Also consdier using regularisations such as Drop-Out.\n",
    "\n",
    "It is strongly suggested that for this task, due to the low number of training images, pre-training on a larger dataset *(a form of 'transfer learning')* should be implemented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## **Building a New CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Block Using U-Net Style Concatenation\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels + skip_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.gn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.gn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        # Upsample x to Match Skip Spatial Size\n",
    "        x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Concatenate Along Channels\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.relu1(self.gn1(self.conv1(x)))\n",
    "        x = self.relu2(self.gn2(self.conv2(x)))\n",
    "        \n",
    "        # Return th Result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterSegNetwork(nn.Module):\n",
    "    def __init__(self, n_class=19):\n",
    "        super(BetterSegNetwork, self).__init__()\n",
    "\n",
    "        # ----------------------------- Build a Custom Encoder -------------------------------\n",
    "        # Stage 1\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.gn1_1 = nn.BatchNorm2d(64)\n",
    "        self.relu1_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.gn1_2 = nn.BatchNorm2d(64)\n",
    "        self.relu1_2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Pooling Layer 1/2 Scale\n",
    "        self.pool1 = nn.MaxPool2d(2, 2, ceil_mode=True) # --> 1/2\n",
    "\n",
    "        # Residual Conv for Stage 1 (input channels 3 → 64)\n",
    "        self.res_conv1 = nn.Conv2d(3, 64, kernel_size=1)  \n",
    "\n",
    "        # Stage 2\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.gn2_1 = nn.BatchNorm2d(128)\n",
    "        self.relu2_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.gn2_2 = nn.BatchNorm2d(128)\n",
    "        self.relu2_2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Pooling Layer 1/2 Scale\n",
    "        self.pool2 = nn.MaxPool2d(2, 2, ceil_mode=True) # --> 1/4\n",
    "\n",
    "        # Residual Conv for Stage 2 (input channels 64 → 128)\n",
    "        self.res_conv2 = nn.Conv2d(64, 128, kernel_size=1)\n",
    "\n",
    "        # Stage 3\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.gn3_1 = nn.BatchNorm2d(256)\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.gn3_2 = nn.BatchNorm2d(256)\n",
    "        self.relu3_2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.gn3_3 = nn.BatchNorm2d(256)\n",
    "        self.relu3_3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Pooling Layer 1/2 Scale\n",
    "        self.pool3 = nn.MaxPool2d(2, 2, ceil_mode=True) # --> 1/8\n",
    "\n",
    "        # Residual Conv for Stage 3 (input channels 128 → 256)\n",
    "        self.res_conv3 = nn.Conv2d(128, 256, kernel_size=1)\n",
    "\n",
    "        # Stage 4\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.gn4_1 = nn.BatchNorm2d(512)\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.drop4_1 = nn.Dropout2d(0.1)\n",
    "\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.gn4_2 = nn.BatchNorm2d(512)\n",
    "        self.relu4_2 = nn.ReLU(inplace=True)\n",
    "        self.drop4_2 = nn.Dropout2d(0.1)\n",
    "\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.gn4_3 = nn.BatchNorm2d(512)\n",
    "        self.relu4_3 = nn.ReLU(inplace=True)\n",
    "        self.drop4_3 = nn.Dropout2d(0.1)\n",
    "\n",
    "        # Pooling Layer 1/2 Scale\n",
    "        self.pool4 = nn.MaxPool2d(2, 2, ceil_mode=True) # --> 1/16\n",
    "\n",
    "        # Residual Conv for Stage 4 (input channels 256 → 512)\n",
    "        self.res_conv4 = nn.Conv2d(256, 512, kernel_size=1)\n",
    "\n",
    "        # Stage 5\n",
    "        self.conv5_1 = nn.Conv2d(512, 1024, 3, padding=2, dilation=2)  # Dilation = 2 expands receptive field\n",
    "        self.gn5_1 = nn.BatchNorm2d(1024)\n",
    "        self.relu5_1 = nn.ReLU(inplace=True)\n",
    "        self.drop5_1 = nn.Dropout2d(0.2)\n",
    "\n",
    "        self.conv5_2 = nn.Conv2d(1024, 1024, 3, padding=2, dilation=2)\n",
    "        self.gn5_2 = nn.BatchNorm2d(1024)\n",
    "        self.relu5_2 = nn.ReLU(inplace=True)\n",
    "        self.drop5_2 = nn.Dropout2d(0.2)\n",
    "\n",
    "        self.conv5_3 = nn.Conv2d(1024, 1024, 3, padding=2, dilation=2)\n",
    "        self.gn5_3 = nn.BatchNorm2d(1024)\n",
    "        self.relu5_3 = nn.ReLU(inplace=True)\n",
    "        self.drop5_3 = nn.Dropout2d(0.2)\n",
    "\n",
    "        # Pooling Layer 1/2 Scale\n",
    "        self.pool5 = nn.MaxPool2d(2, 2, ceil_mode=True) \n",
    "\n",
    "        # Residual Conv for Stage 5 (input channels 512 → 1024)\n",
    "        self.res_conv5 = nn.Conv2d(512, 1024, kernel_size=1)\n",
    "        # --------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        # Center Block (Switch Direction)\n",
    "        self.center_conv1 = nn.Conv2d(1024, 512, 3, padding=4, dilation=4)  # Bigger Dilation Here\n",
    "        self.center_gn1 = nn.BatchNorm2d(512)\n",
    "        self.center_relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.center_conv2 = nn.Conv2d(512, 512, 3, padding=4, dilation=4)\n",
    "        self.center_gn2 = nn.BatchNorm2d(512)\n",
    "        self.center_relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "        # ------------------------------- Build a Custom Decoder -----------------------------------------\n",
    "        self.dec5 = DecoderBlock(in_channels=512, skip_channels=512, out_channels=512)  # 512 + 512 -> 512\n",
    "        self.dec4 = DecoderBlock(in_channels=512, skip_channels=256, out_channels=256)  # 512 + 256 -> 256\n",
    "        self.dec3 = DecoderBlock(in_channels=256, skip_channels=128, out_channels=128)  # 256 + 128 -> 128\n",
    "        self.dec2 = DecoderBlock(in_channels=128, skip_channels=64, out_channels=64)    # 128 + 64 -> 64\n",
    "        # ------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        # Final Segmentation Head: Predict n_class Channels\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1),  # Helps Generalise with Small Datasets\n",
    "\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1),  # Helps Generalise with Small Datasets\n",
    "\n",
    "            nn.Conv2d(32, n_class, kernel_size=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stage 1\n",
    "        residual = self.res_conv1(x)\n",
    "        out = self.relu1_1(self.gn1_1(self.conv1_1(x)))\n",
    "        out = self.relu1_2(self.gn1_2(self.conv1_2(out)))\n",
    "        x = out + residual  # Residual Connection\n",
    "        c1 = self.pool1(x)  # --> 1/2\n",
    "\n",
    "        # Stage 2\n",
    "        residual = self.res_conv2(c1)\n",
    "        out = self.relu2_1(self.gn2_1(self.conv2_1(c1)))\n",
    "        out = self.relu2_2(self.gn2_2(self.conv2_2(out)))\n",
    "        x = out + residual  # Residual Connection\n",
    "        c2 = self.pool2(x)  # --> 1/4\n",
    "\n",
    "        # Stage 3\n",
    "        residual = self.res_conv3(c2)\n",
    "        out = self.relu3_1(self.gn3_1(self.conv3_1(c2)))\n",
    "        out = self.relu3_2(self.gn3_2(self.conv3_2(out)))\n",
    "        out = self.relu3_3(self.gn3_3(self.conv3_3(out)))\n",
    "        x = out + residual  # Residual Connection\n",
    "        c3 = self.pool3(x)  # --> 1/8\n",
    "\n",
    "        # Stage 4\n",
    "        residual = self.res_conv4(c3)\n",
    "        out = self.drop4_1(self.relu4_1(self.gn4_1(self.conv4_1(c3))))\n",
    "        out = self.drop4_2(self.relu4_2(self.gn4_2(self.conv4_2(out))))\n",
    "        out = self.drop4_3(self.relu4_3(self.gn4_3(self.conv4_3(out))))\n",
    "        x = out + residual  # Residual Connection\n",
    "        c4 = self.pool4(x)  # --> 1/16\n",
    "\n",
    "        # Stage 5\n",
    "        residual = self.res_conv5(c4)\n",
    "        out = self.drop5_1(self.relu5_1(self.gn5_1(self.conv5_1(c4))))\n",
    "        out = self.drop5_2(self.relu5_2(self.gn5_2(self.conv5_2(out))))\n",
    "        out = self.drop5_3(self.relu5_3(self.gn5_3(self.conv5_3(out))))\n",
    "        x = out + residual  # Residual Connection\n",
    "\n",
    "        # Center (Switch Direction)\n",
    "        x = self.center_relu1(self.center_gn1(self.center_conv1(x)))\n",
    "        x = self.center_relu2(self.center_gn2(self.center_conv2(x)))\n",
    "\n",
    "        # Decoder with Concatenation\n",
    "        x = self.dec5(x, c4)  # 1/16 scale\n",
    "        x = self.dec4(x, c3)  # 1/8 scale\n",
    "        x = self.dec3(x, c2)  # 1/4 scale\n",
    "        x = self.dec2(x, c1)  # 1/2 scale\n",
    "\n",
    "        # Final Upsample to Original Input Size (assumes input downscaled 1/2)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Final Segmentation Prediction\n",
    "        out = self.segmentation_head(x)  # (B, n_class, H, W)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained weights found at ./Models/pretrained_weights.pth, starting from scratch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BetterSegNetwork                         [1, 19, 256, 864]         --\n",
       "├─Conv2d: 1-1                            [1, 64, 256, 864]         256\n",
       "├─Conv2d: 1-2                            [1, 64, 256, 864]         1,792\n",
       "├─BatchNorm2d: 1-3                       [1, 64, 256, 864]         128\n",
       "├─ReLU: 1-4                              [1, 64, 256, 864]         --\n",
       "├─Conv2d: 1-5                            [1, 64, 256, 864]         36,928\n",
       "├─BatchNorm2d: 1-6                       [1, 64, 256, 864]         128\n",
       "├─ReLU: 1-7                              [1, 64, 256, 864]         --\n",
       "├─MaxPool2d: 1-8                         [1, 64, 128, 432]         --\n",
       "├─Conv2d: 1-9                            [1, 128, 128, 432]        8,320\n",
       "├─Conv2d: 1-10                           [1, 128, 128, 432]        73,856\n",
       "├─BatchNorm2d: 1-11                      [1, 128, 128, 432]        256\n",
       "├─ReLU: 1-12                             [1, 128, 128, 432]        --\n",
       "├─Conv2d: 1-13                           [1, 128, 128, 432]        147,584\n",
       "├─BatchNorm2d: 1-14                      [1, 128, 128, 432]        256\n",
       "├─ReLU: 1-15                             [1, 128, 128, 432]        --\n",
       "├─MaxPool2d: 1-16                        [1, 128, 64, 216]         --\n",
       "├─Conv2d: 1-17                           [1, 256, 64, 216]         33,024\n",
       "├─Conv2d: 1-18                           [1, 256, 64, 216]         295,168\n",
       "├─BatchNorm2d: 1-19                      [1, 256, 64, 216]         512\n",
       "├─ReLU: 1-20                             [1, 256, 64, 216]         --\n",
       "├─Conv2d: 1-21                           [1, 256, 64, 216]         590,080\n",
       "├─BatchNorm2d: 1-22                      [1, 256, 64, 216]         512\n",
       "├─ReLU: 1-23                             [1, 256, 64, 216]         --\n",
       "├─Conv2d: 1-24                           [1, 256, 64, 216]         590,080\n",
       "├─BatchNorm2d: 1-25                      [1, 256, 64, 216]         512\n",
       "├─ReLU: 1-26                             [1, 256, 64, 216]         --\n",
       "├─MaxPool2d: 1-27                        [1, 256, 32, 108]         --\n",
       "├─Conv2d: 1-28                           [1, 512, 32, 108]         131,584\n",
       "├─Conv2d: 1-29                           [1, 512, 32, 108]         1,180,160\n",
       "├─BatchNorm2d: 1-30                      [1, 512, 32, 108]         1,024\n",
       "├─ReLU: 1-31                             [1, 512, 32, 108]         --\n",
       "├─Dropout2d: 1-32                        [1, 512, 32, 108]         --\n",
       "├─Conv2d: 1-33                           [1, 512, 32, 108]         2,359,808\n",
       "├─BatchNorm2d: 1-34                      [1, 512, 32, 108]         1,024\n",
       "├─ReLU: 1-35                             [1, 512, 32, 108]         --\n",
       "├─Dropout2d: 1-36                        [1, 512, 32, 108]         --\n",
       "├─Conv2d: 1-37                           [1, 512, 32, 108]         2,359,808\n",
       "├─BatchNorm2d: 1-38                      [1, 512, 32, 108]         1,024\n",
       "├─ReLU: 1-39                             [1, 512, 32, 108]         --\n",
       "├─Dropout2d: 1-40                        [1, 512, 32, 108]         --\n",
       "├─MaxPool2d: 1-41                        [1, 512, 16, 54]          --\n",
       "├─Conv2d: 1-42                           [1, 1024, 16, 54]         525,312\n",
       "├─Conv2d: 1-43                           [1, 1024, 16, 54]         4,719,616\n",
       "├─BatchNorm2d: 1-44                      [1, 1024, 16, 54]         2,048\n",
       "├─ReLU: 1-45                             [1, 1024, 16, 54]         --\n",
       "├─Dropout2d: 1-46                        [1, 1024, 16, 54]         --\n",
       "├─Conv2d: 1-47                           [1, 1024, 16, 54]         9,438,208\n",
       "├─BatchNorm2d: 1-48                      [1, 1024, 16, 54]         2,048\n",
       "├─ReLU: 1-49                             [1, 1024, 16, 54]         --\n",
       "├─Dropout2d: 1-50                        [1, 1024, 16, 54]         --\n",
       "├─Conv2d: 1-51                           [1, 1024, 16, 54]         9,438,208\n",
       "├─BatchNorm2d: 1-52                      [1, 1024, 16, 54]         2,048\n",
       "├─ReLU: 1-53                             [1, 1024, 16, 54]         --\n",
       "├─Dropout2d: 1-54                        [1, 1024, 16, 54]         --\n",
       "├─Conv2d: 1-55                           [1, 512, 16, 54]          4,719,104\n",
       "├─BatchNorm2d: 1-56                      [1, 512, 16, 54]          1,024\n",
       "├─ReLU: 1-57                             [1, 512, 16, 54]          --\n",
       "├─Conv2d: 1-58                           [1, 512, 16, 54]          2,359,808\n",
       "├─BatchNorm2d: 1-59                      [1, 512, 16, 54]          1,024\n",
       "├─ReLU: 1-60                             [1, 512, 16, 54]          --\n",
       "├─DecoderBlock: 1-61                     [1, 512, 16, 54]          --\n",
       "│    └─Conv2d: 2-1                       [1, 512, 16, 54]          4,719,104\n",
       "│    └─BatchNorm2d: 2-2                  [1, 512, 16, 54]          1,024\n",
       "│    └─ReLU: 2-3                         [1, 512, 16, 54]          --\n",
       "│    └─Conv2d: 2-4                       [1, 512, 16, 54]          2,359,808\n",
       "│    └─BatchNorm2d: 2-5                  [1, 512, 16, 54]          1,024\n",
       "│    └─ReLU: 2-6                         [1, 512, 16, 54]          --\n",
       "├─DecoderBlock: 1-62                     [1, 256, 32, 108]         --\n",
       "│    └─Conv2d: 2-7                       [1, 256, 32, 108]         1,769,728\n",
       "│    └─BatchNorm2d: 2-8                  [1, 256, 32, 108]         512\n",
       "│    └─ReLU: 2-9                         [1, 256, 32, 108]         --\n",
       "│    └─Conv2d: 2-10                      [1, 256, 32, 108]         590,080\n",
       "│    └─BatchNorm2d: 2-11                 [1, 256, 32, 108]         512\n",
       "│    └─ReLU: 2-12                        [1, 256, 32, 108]         --\n",
       "├─DecoderBlock: 1-63                     [1, 128, 64, 216]         --\n",
       "│    └─Conv2d: 2-13                      [1, 128, 64, 216]         442,496\n",
       "│    └─BatchNorm2d: 2-14                 [1, 128, 64, 216]         256\n",
       "│    └─ReLU: 2-15                        [1, 128, 64, 216]         --\n",
       "│    └─Conv2d: 2-16                      [1, 128, 64, 216]         147,584\n",
       "│    └─BatchNorm2d: 2-17                 [1, 128, 64, 216]         256\n",
       "│    └─ReLU: 2-18                        [1, 128, 64, 216]         --\n",
       "├─DecoderBlock: 1-64                     [1, 64, 128, 432]         --\n",
       "│    └─Conv2d: 2-19                      [1, 64, 128, 432]         110,656\n",
       "│    └─BatchNorm2d: 2-20                 [1, 64, 128, 432]         128\n",
       "│    └─ReLU: 2-21                        [1, 64, 128, 432]         --\n",
       "│    └─Conv2d: 2-22                      [1, 64, 128, 432]         36,928\n",
       "│    └─BatchNorm2d: 2-23                 [1, 64, 128, 432]         128\n",
       "│    └─ReLU: 2-24                        [1, 64, 128, 432]         --\n",
       "├─Sequential: 1-65                       [1, 19, 256, 864]         --\n",
       "│    └─Conv2d: 2-25                      [1, 64, 256, 864]         36,928\n",
       "│    └─BatchNorm2d: 2-26                 [1, 64, 256, 864]         128\n",
       "│    └─ReLU: 2-27                        [1, 64, 256, 864]         --\n",
       "│    └─Conv2d: 2-28                      [1, 64, 256, 864]         36,928\n",
       "│    └─BatchNorm2d: 2-29                 [1, 64, 256, 864]         128\n",
       "│    └─ReLU: 2-30                        [1, 64, 256, 864]         --\n",
       "│    └─Dropout2d: 2-31                   [1, 64, 256, 864]         --\n",
       "│    └─Conv2d: 2-32                      [1, 32, 256, 864]         18,464\n",
       "│    └─BatchNorm2d: 2-33                 [1, 32, 256, 864]         64\n",
       "│    └─ReLU: 2-34                        [1, 32, 256, 864]         --\n",
       "│    └─Conv2d: 2-35                      [1, 32, 256, 864]         9,248\n",
       "│    └─BatchNorm2d: 2-36                 [1, 32, 256, 864]         64\n",
       "│    └─ReLU: 2-37                        [1, 32, 256, 864]         --\n",
       "│    └─Dropout2d: 2-38                   [1, 32, 256, 864]         --\n",
       "│    └─Conv2d: 2-39                      [1, 19, 256, 864]         627\n",
       "==========================================================================================\n",
       "Total params: 49,305,075\n",
       "Trainable params: 49,305,075\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 143.17\n",
       "==========================================================================================\n",
       "Input size (MB): 2.65\n",
       "Forward/backward pass size (MB): 2135.75\n",
       "Params size (MB): 197.22\n",
       "Estimated Total Size (MB): 2335.63\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Model Instance (must match the architecture exactly)\n",
    "segNet = BetterSegNetwork(n_class=19).to(device)\n",
    "\n",
    "# Load the Pretrained Weights\n",
    "weights_path = './Models/pretrained_weights.pth'\n",
    "\n",
    "# Check that there are Weights Avaliable\n",
    "if os.path.isfile(weights_path):\n",
    "    print(f\"Loading pretrained weights from {weights_path}\")\n",
    "    segNet.load_state_dict(torch.load(weights_path))\n",
    "else:\n",
    "    print(f\"No pretrained weights found at {weights_path}, starting from scratch.\")\n",
    "\n",
    "# Print Model Summary With Input Size (batch_size, channels, height, width)\n",
    "summary(segNet, input_size=(1, 3, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## **Making a Pre-Trained Hybrid Model**\n",
    "\n",
    "The model architecture consists of EfficientNet-B0 as the encoder backbone, leveraging weights pretrained on ImageNet for better feature extraction. This encoder is paired with the DeepLabV3 decoder, which performs multi-scale context aggregation for accurate semantic segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained weights found at ./Models/pretrained_weights.pth, starting from scratch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "Segformer                                               [1, 19, 256, 864]         --\n",
       "├─EfficientNetEncoder: 1-1                              [1, 3, 256, 864]          592,896\n",
       "│    └─Conv2dStaticSamePadding: 2-1                     [1, 40, 128, 432]         1,080\n",
       "│    │    └─ZeroPad2d: 3-1                              [1, 3, 257, 865]          --\n",
       "│    └─BatchNorm2d: 2-2                                 [1, 40, 128, 432]         80\n",
       "│    └─SiLU: 2-3                                        [1, 40, 128, 432]         --\n",
       "│    └─ModuleList: 2-4                                  --                        --\n",
       "│    │    └─MBConvBlock: 3-2                            [1, 24, 128, 432]         2,298\n",
       "│    │    └─MBConvBlock: 3-3                            [1, 24, 128, 432]         1,206\n",
       "│    │    └─MBConvBlock: 3-4                            [1, 32, 64, 216]          11,878\n",
       "│    │    └─MBConvBlock: 3-5                            [1, 32, 64, 216]          18,120\n",
       "│    │    └─MBConvBlock: 3-6                            [1, 32, 64, 216]          18,120\n",
       "│    │    └─MBConvBlock: 3-7                            [1, 48, 32, 108]          24,296\n",
       "│    │    └─MBConvBlock: 3-8                            [1, 48, 32, 108]          43,308\n",
       "│    │    └─MBConvBlock: 3-9                            [1, 48, 32, 108]          43,308\n",
       "│    │    └─MBConvBlock: 3-10                           [1, 96, 16, 54]           52,620\n",
       "│    │    └─MBConvBlock: 3-11                           [1, 96, 16, 54]           146,520\n",
       "│    │    └─MBConvBlock: 3-12                           [1, 96, 16, 54]           146,520\n",
       "│    │    └─MBConvBlock: 3-13                           [1, 96, 16, 54]           146,520\n",
       "│    │    └─MBConvBlock: 3-14                           [1, 96, 16, 54]           146,520\n",
       "│    │    └─MBConvBlock: 3-15                           [1, 136, 16, 54]          178,856\n",
       "│    │    └─MBConvBlock: 3-16                           [1, 136, 16, 54]          302,226\n",
       "│    │    └─MBConvBlock: 3-17                           [1, 136, 16, 54]          302,226\n",
       "│    │    └─MBConvBlock: 3-18                           [1, 136, 16, 54]          302,226\n",
       "│    │    └─MBConvBlock: 3-19                           [1, 136, 16, 54]          302,226\n",
       "│    │    └─MBConvBlock: 3-20                           [1, 232, 8, 27]           380,754\n",
       "│    │    └─MBConvBlock: 3-21                           [1, 232, 8, 27]           849,642\n",
       "│    │    └─MBConvBlock: 3-22                           [1, 232, 8, 27]           849,642\n",
       "│    │    └─MBConvBlock: 3-23                           [1, 232, 8, 27]           849,642\n",
       "│    │    └─MBConvBlock: 3-24                           [1, 232, 8, 27]           849,642\n",
       "│    │    └─MBConvBlock: 3-25                           [1, 232, 8, 27]           849,642\n",
       "│    │    └─MBConvBlock: 3-26                           [1, 384, 8, 27]           1,039,258\n",
       "│    │    └─MBConvBlock: 3-27                           [1, 384, 8, 27]           2,244,960\n",
       "├─SegformerDecoder: 1-2                                 [1, 256, 64, 216]         --\n",
       "│    └─ModuleList: 2-5                                  --                        --\n",
       "│    │    └─MLP: 3-28                                   [1, 256, 8, 27]           98,560\n",
       "│    │    └─MLP: 3-29                                   [1, 256, 16, 54]          35,072\n",
       "│    │    └─MLP: 3-30                                   [1, 256, 32, 108]         12,544\n",
       "│    │    └─MLP: 3-31                                   [1, 256, 64, 216]         8,448\n",
       "│    │    └─MLP: 3-32                                   [1, 256, 128, 432]        10,496\n",
       "│    └─Conv2dReLU: 2-6                                  [1, 256, 64, 216]         --\n",
       "│    │    └─Conv2d: 3-33                                [1, 256, 64, 216]         327,680\n",
       "│    │    └─BatchNorm2d: 3-34                           [1, 256, 64, 216]         512\n",
       "│    │    └─ReLU: 3-35                                  [1, 256, 64, 216]         --\n",
       "├─SegmentationHead: 1-3                                 [1, 19, 256, 864]         --\n",
       "│    └─Conv2d: 2-7                                      [1, 19, 64, 216]          4,883\n",
       "│    └─UpsamplingBilinear2d: 2-8                        [1, 19, 256, 864]         --\n",
       "│    └─Activation: 2-9                                  [1, 19, 256, 864]         --\n",
       "│    │    └─Identity: 3-36                              [1, 19, 256, 864]         --\n",
       "=========================================================================================================\n",
       "Total params: 11,194,427\n",
       "Trainable params: 11,194,427\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 4.60\n",
       "=========================================================================================================\n",
       "Input size (MB): 2.65\n",
       "Forward/backward pass size (MB): 664.88\n",
       "Params size (MB): 2.33\n",
       "Estimated Total Size (MB): 669.86\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device Config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create Model Instance using EfficientNet-B0 + DeepLabV3+\n",
    "segNet = smp.Segformer(\n",
    "    encoder_name=\"efficientnet-b3\",   # EfficientNet-B0 backbone\n",
    "    encoder_weights=\"imagenet\",       # Pretrained on ImageNet\n",
    "    in_channels=3,                    # RGB Images\n",
    "    classes=19                        # Output Segmentation Classes\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Check that there are Weights Avaliable\n",
    "if os.path.isfile(weights_path):\n",
    "    print(f\"Loading pretrained weights from {weights_path}\")\n",
    "\n",
    "    # Load the Pre-trained Weights\n",
    "    checkpoint_path = './Models/PreTrained_Seg_B3.pth'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Handle Potential Key Prefixes (like 'module.')\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in checkpoint.items():\n",
    "        name = k\n",
    "\n",
    "        # Remove 'module.' Prefix if Present\n",
    "        if k.startswith('module.'):\n",
    "            name = k[7:]  \n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    # Load Weights into Model\n",
    "    segNet.load_state_dict(new_state_dict)\n",
    "else:\n",
    "    print(f\"No pretrained weights found at {weights_path}, starting from scratch.\")\n",
    "\n",
    "\n",
    "# Print Model Summary\n",
    "summary(segNet, input_size=(1, 3, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "## **Loss & Optimisation**\n",
    "\n",
    "The model is trained using a custom Multi-Class Dice Loss function to effectively handle segmentation tasks. Optimisation is performed with the Adam optimiser incorporating weight decay for regularisation. Additionally, a Cosine Annealing learning rate scheduler is employed to progressively decrease the learning rate over epochs, promoting better convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- Build a Custom Loss Function ----------------------------------------\n",
    "class MulticlassDiceLoss(nn.Module):\n",
    "    def __init__(self, num_classes: int, eps: float = 1e-7, beta: float = 1.0, reduction: str = \"sum\"):\n",
    "        super(MulticlassDiceLoss, self).__init__()\n",
    "        # Select Device: GPU if Available Else CPU\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Create a Tensor of Ones with Length = Num_classes, Used for Loss Calculation\n",
    "        self.ones_tensor: torch.Tensor = torch.ones(num_classes, device=self.device)\n",
    "        \n",
    "        # Sum of Ones Tensor, Effectively the Number of Classes as a Float Tensor\n",
    "        self.num_classes = torch.sum(self.ones_tensor)\n",
    "        \n",
    "        # Store Integer Number of Classes for Use in One-Hot Encoding & Masking\n",
    "        self.num_classes_int = num_classes\n",
    "        \n",
    "        # Store the Reduction Method in Lowercase for Later Use\n",
    "        self.reduction: str = reduction.lower()\n",
    "        \n",
    "        # Small Epsilon to Prevent Division by Zero in Calculations\n",
    "        self.eps: float = eps\n",
    "        \n",
    "        # Square of Beta, Used in F-score Calculation to Weigh Precision Recall\n",
    "        self.beta2: float = beta ** 2\n",
    "\n",
    "\n",
    "    def multiclass_f_score(self, gt: torch.Tensor, pr: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply Softmax to Predictions to Get Class Probabilities Along Channel Dimension\n",
    "        pr_softmax = torch.softmax(pr, dim=1)  # Shape: (B, C, H, W)\n",
    "\n",
    "        # Replace Ignore Label (255) in Ground Truth With num_classes (to exclude in one-hot)\n",
    "        gt[gt == 255] = self.num_classes_int\n",
    "\n",
    "        # Convert Ground Truth to One-hot Encoding Including an Extra Class for Ignored Pixels\n",
    "        gt_one_hot = torch.nn.functional.one_hot(gt, num_classes=self.num_classes_int + 1)  # (B, H, W, C+1)\n",
    "        \n",
    "        # Exclude the Ignored Class by Slicing\n",
    "        gt_one_hot = gt_one_hot[:, :, :, :self.num_classes_int].float()\n",
    "        \n",
    "        # Change Shape from (B, H, W, C) to (B, C, H, W) to Align with Predictions\n",
    "        gt_one_hot = gt_one_hot.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "\n",
    "        # True Positives: Sum Over Batch & Spatial Dims where Prediction & GT Agree\n",
    "        tp = torch.sum(gt_one_hot * pr_softmax, dim=(0, 2, 3))\n",
    "        \n",
    "        # False Positives: Predicted Positives Minus True Positives\n",
    "        fp = torch.sum(pr_softmax, dim=(0, 2, 3)) - tp\n",
    "        \n",
    "        # False Negatives: Actual Positives Minus True Positives\n",
    "        fn = torch.sum(gt_one_hot, dim=(0, 2, 3)) - tp\n",
    "\n",
    "        # Calculate F-score with Smoothing (eps) & Beta Weighting\n",
    "        return ((1 + self.beta2) * tp + self.eps) / ((1 + self.beta2) * tp + self.beta2 * fn + fp + self.eps)\n",
    "\n",
    "\n",
    "    def forward(self, pr: torch.Tensor, gt: torch.Tensor) -> torch.Tensor:\n",
    "        # Calculate F-score for Each Class\n",
    "        f_score = self.multiclass_f_score(pr=pr, gt=gt)\n",
    "        \n",
    "        # Dice Loss is 1 - F-score\n",
    "        loss = self.ones_tensor.to(f_score.device) - f_score\n",
    "\n",
    "        # Apply Specified Reduction to the Loss Tensor\n",
    "        if self.reduction == \"none\":\n",
    "            # No Reduction, Return Per-Class Loss\n",
    "            pass\n",
    "        elif self.reduction == \"mean\":\n",
    "            # Mean Loss Over All Classes\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            # Sum Loss Over All Classes, Cast to float32 for Numerical Stability\n",
    "            loss = loss.sum(dtype=torch.float32)\n",
    "        elif self.reduction == \"batchwise_mean\":\n",
    "            # Sum Over Batch Dimension & Average Per Class (if Loss is Batchwise)\n",
    "            loss = loss.sum(dim=0, dtype=torch.float32)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1747833729540,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "ubLuJmG7XXcV"
   },
   "outputs": [],
   "source": [
    "# Define the Loss Function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "# Define the Optimiser\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=segNet.parameters(), \n",
    "    lr=learning_rate,      # Lower Learning Rate to Avoid Overfitting\n",
    "    weight_decay=1e-4      # Helps Prevent Overfitting (Regularisation)\n",
    ")\n",
    "\n",
    "# Define the Leanring Rate Scheduler\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=epochs, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzUEUtY6aXMG"
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "# **The Function Used to Compare the Precision**\n",
    "\n",
    "**DO NOT MODIFY THIS CODE!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747833729540,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "2rFg1PNNa3Ch"
   },
   "outputs": [],
   "source": [
    "def cal_acc(pred_folder, gt_folder, classes=19):\n",
    "    class AverageMeter(object):\n",
    "        def __init__(self):\n",
    "            self.reset()\n",
    "        def reset(self):\n",
    "            self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "        def update(self, val, n=1):\n",
    "            self.val = val\n",
    "            self.sum += val * n\n",
    "            self.count += n\n",
    "            self.avg = self.sum / self.count\n",
    "    def intersectionAndUnion(output, target, K, ignore_index=255):\n",
    "        assert (output.ndim in [1, 2, 3])\n",
    "        assert output.shape == target.shape\n",
    "        output = output.reshape(output.size).copy()\n",
    "        target = target.reshape(target.size)\n",
    "        output[np.where(target == ignore_index)[0]] = ignore_index\n",
    "        intersection = output[np.where(output == target)[0]]\n",
    "        area_intersection, _ = np.histogram(intersection, bins=np.arange(K + 1))\n",
    "        area_output, _ = np.histogram(output, bins=np.arange(K + 1))\n",
    "        area_target, _ = np.histogram(target, bins=np.arange(K + 1))\n",
    "        area_union = area_output + area_target - area_intersection\n",
    "        return area_intersection, area_union, area_target\n",
    "    data_list = os.listdir(gt_folder)\n",
    "    intersection_meter = AverageMeter()\n",
    "    union_meter = AverageMeter()\n",
    "    target_meter = AverageMeter()\n",
    "    for i, image_name in enumerate(data_list):\n",
    "        pred = cv2.imread(os.path.join(pred_folder, image_name), cv2.IMREAD_GRAYSCALE)\n",
    "        target = cv2.imread(os.path.join(gt_folder, image_name), cv2.IMREAD_GRAYSCALE)\n",
    "        intersection, union, target = intersectionAndUnion(pred, target, classes)\n",
    "        intersection_meter.update(intersection)\n",
    "        union_meter.update(union)\n",
    "        target_meter.update(target)\n",
    "    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n",
    "    mIoU = np.mean(iou_class)\n",
    "    print('Eval result: mIoU {:.4f}.'.format(mIoU))\n",
    "    return mIoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2dRJ8TgboP9"
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "# **Define Functions to Get & Save Predictions**\n",
    "\n",
    "Multi-scale testing can be used here to reduce the number of training cycles needed to evaluate model parameters' effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747833729541,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "dBcRAC2AhLzS"
   },
   "outputs": [],
   "source": [
    "# Create a Directory if it Does Not Already Exist\n",
    "def make_folder(dir_name): \n",
    "    if not os.path.exists(dir_name):\n",
    "        # Create the Directory & Any Necessary Parent Directories\n",
    "        os.makedirs(dir_name)  \n",
    "\n",
    "\n",
    "# Move All Files From Temporary Prediction Folders to Result Folders & Remove Temporary Folders\n",
    "def move_folders(grey_temp, color_temp, grey_rs, color_rs):\n",
    "    # If Temporary Grayscale Prediction Folder Exists\n",
    "    if os.path.exists(grey_temp):\n",
    "        # Ensure Destination Folder Exists\n",
    "        make_folder(grey_rs)  \n",
    "\n",
    "        # Move Each File From Temp to Result Directory\n",
    "        for file in os.listdir(grey_temp):\n",
    "            shutil.move(os.path.join(grey_temp, file), os.path.join(grey_rs, file))\n",
    "\n",
    "        # Remove the Temp Folder if it Still Exists After Moving\n",
    "        if os.path.exists(grey_temp):\n",
    "            shutil.rmtree(grey_temp)\n",
    "\n",
    "    # Same Process for the Colour Prediction Folder\n",
    "    if os.path.exists(color_temp):\n",
    "        make_folder(color_rs)\n",
    "        for file in os.listdir(color_temp):\n",
    "            shutil.move(os.path.join(color_temp, file), os.path.join(color_rs, file))\n",
    "        if os.path.exists(color_temp):\n",
    "            shutil.rmtree(color_temp)\n",
    "\n",
    "\n",
    "# Convert a Grayscale Mask (With Class Indices) to a Colorised Image Using a Palette\n",
    "def colorize(gray, palette):\n",
    "    # Convert the Grayscale NumPy Array to a PIL Image in 'P' Mode (Palette-Based)\n",
    "    color = Image.fromarray(gray.astype(np.uint8)).convert('P')\n",
    "    \n",
    "    # Apply the Palette (A Flat List of RGB Triplets For Each Class Index)\n",
    "    color.putpalette(palette)\n",
    "\n",
    "    # Return the Colorised Image\n",
    "    return color\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------- PERFORM EVALUATION FOR A NETWORK & SAVE PREDICTION RESULTS -----------------------\n",
    "def get_predictions(segNet, dataFolder, device):\n",
    "    # Define Folders to Save Grayscale & Colour Predictions\n",
    "    gray_folder, color_folder, model_folder = './TempFiles/temp_grey', './TempFiles/temp_color', './Models'\n",
    "    \n",
    "    # List All Image Filenames in the Testing Images Folder\n",
    "    listImages = os.listdir(os.path.join(dataFolder, \"testing/image\"))\n",
    "    \n",
    "    # Define the Path to Ground Truth Label Folder\n",
    "    gt_folder = os.path.join(dataFolder, \"testing/label\")\n",
    "    \n",
    "    # Path to Colour Palette for Visualisation\n",
    "    colors_path  = os.path.join(dataFolder, \"colors.txt\")\n",
    "    print('Begin Testing')\n",
    "    \n",
    "    # Create Folders if They Don't Exist\n",
    "    make_folder(gray_folder)\n",
    "    make_folder(color_folder)\n",
    "    make_folder(model_folder)\n",
    "    \n",
    "    # Load Colours for Visualising Grayscale Masks as RGB\n",
    "    colors = np.loadtxt(colors_path).astype('uint8')\n",
    "\n",
    "    # Albumentations Transform with Resize + Normalise + Tensor\n",
    "    transformTest = A.Compose([\n",
    "        A.Resize(height, width, interpolation=cv2.INTER_LINEAR),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    # Make Sure Model is in Eval Mode\n",
    "    segNet.eval()  \n",
    "\n",
    "    # Iterate Over All Images to Predict\n",
    "    with torch.no_grad():\n",
    "        for img_name in listImages:\n",
    "            img_path = os.path.join(dataFolder, \"testing/image\", img_name)\n",
    "            img = cv2.imread(img_path)[:, :, ::-1]  # BGR to RGB\n",
    "            \n",
    "            # Apply Albumentations transform\n",
    "            augmented = transformTest(image=img)\n",
    "            img_tensor = augmented['image'].unsqueeze(0).to(device)  # (1,3,H,W)\n",
    "\n",
    "            prediction = segNet(img_tensor)[0].cpu().numpy()\n",
    "            prediction = np.argmax(prediction, axis=0)  # (H, W)\n",
    "\n",
    "            gt_mask = cv2.imread(os.path.join(gt_folder, img_name), cv2.IMREAD_GRAYSCALE)\n",
    "            gt_h, gt_w = gt_mask.shape\n",
    "\n",
    "            # Resize prediction to original GT size using nearest neighbor\n",
    "            prediction_resized = cv2.resize(prediction, (gt_w, gt_h), interpolation=cv2.INTER_NEAREST)\n",
    "            gray = np.uint8(prediction_resized)\n",
    "            color = colorize(gray, colors)\n",
    "\n",
    "            gray_path = os.path.join(gray_folder, img_name)\n",
    "            color_path = os.path.join(color_folder, img_name)\n",
    "\n",
    "            cv2.imwrite(gray_path, gray) # type: ignore\n",
    "            color.save(color_path)\n",
    "\n",
    "    return gray_folder, color_folder\n",
    "# -----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqnC6C_YkqRO"
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "# **Train the Network**\n",
    "\n",
    "Training on the original base model will take ~1 hour to complete. It is possible to define the false case in order to save on training time. Remember to download the results before closing the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2117560,
     "status": "ok",
     "timestamp": 1747835847097,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "JvzZ3HxFkt_c",
    "outputId": "b893b385-5257-4cd4-83e1-612bf3f3acff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0 loss=3.1291003227233887\n",
      "epoch 0 iter 1 loss=2.969278573989868\n",
      "epoch 0 iter 2 loss=2.7602453231811523\n",
      "epoch 0 iter 3 loss=2.752054214477539\n",
      "epoch 0 iter 4 loss=2.552300214767456\n",
      "epoch 0 iter 5 loss=2.4534926414489746\n",
      "epoch 0 iter 6 loss=2.2292656898498535\n",
      "epoch 0 iter 7 loss=2.0804648399353027\n",
      "epoch 0 iter 8 loss=2.073598623275757\n",
      "epoch 0 iter 9 loss=1.8954391479492188\n",
      "epoch 0 iter 10 loss=1.774312138557434\n",
      "epoch 0 iter 11 loss=1.9075394868850708\n",
      "epoch 0 iter 12 loss=1.5634236335754395\n",
      "epoch 0 iter 13 loss=1.4924960136413574\n",
      "epoch 0 iter 14 loss=1.4116778373718262\n",
      "epoch 0 iter 15 loss=1.5618622303009033\n",
      "epoch 0 iter 16 loss=1.4289894104003906\n",
      "epoch 0 iter 17 loss=1.370937705039978\n",
      "epoch 0 iter 18 loss=1.1861200332641602\n",
      "epoch 0 iter 19 loss=1.260736107826233\n",
      "epoch 0 iter 20 loss=1.3013893365859985\n",
      "epoch 0 iter 21 loss=1.4084367752075195\n",
      "epoch 0 iter 22 loss=1.176146149635315\n",
      "epoch 0 iter 23 loss=0.9793184399604797\n",
      "epoch 0 iter 24 loss=0.9973100423812866\n",
      "epoch 0 iter 25 loss=1.032623291015625\n",
      "epoch 0 iter 26 loss=0.9800511598587036\n",
      "epoch 0 iter 27 loss=1.058090329170227\n",
      "epoch 0 iter 28 loss=0.8503637909889221\n",
      "epoch 0 iter 29 loss=1.0250256061553955\n",
      "epoch 0 iter 30 loss=0.731738805770874\n",
      "epoch 0 iter 31 loss=0.7167646288871765\n",
      "epoch 0 iter 32 loss=0.8441222310066223\n",
      "epoch 0 iter 33 loss=1.0848714113235474\n",
      "epoch 0 iter 34 loss=1.111129641532898\n",
      "epoch 0 iter 35 loss=0.7683391571044922\n",
      "epoch 0 iter 36 loss=0.724388062953949\n",
      "epoch 0 iter 37 loss=1.3900831937789917\n",
      "The final mIoU is : 0.0000.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------ TRAINING LOOP ------------------------------------------\n",
    "mIoU = 0.0             # Track the Best Mean Intersection Over Union (mIoU) Score\n",
    "IoU_list = []          # Store mIoU for Each Evaluation Step\n",
    "train_loss = []        # Store Loss Values for Plotting or Analysis\n",
    "evl_each = True        # Flag to Control Whether Evaluation Happens After Each Epoch\n",
    "\n",
    "# Loop Through the Number of Epochs Defined Earlier\n",
    "for epoch in range(epochs):\n",
    "    # Set Model to Training Mode (Enables Dropout, Batchnorm, etc.)\n",
    "    segNet.train()  \n",
    "\n",
    "    # Iterate Over All Batches in the Training DataLoader\n",
    "    for iter, (imgs, labels) in enumerate(train_loader):\n",
    "        # Forward Pass\n",
    "        pred = segNet(imgs.to(device))                    \n",
    "        \n",
    "        # Reset Gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute Loss\n",
    "        loss = criterion(pred, labels.long().to(device))\n",
    "\n",
    "        # Store Loss\n",
    "        train_loss.append(loss.detach().cpu().numpy().item())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimiser Step\n",
    "        optimizer.step()                      \n",
    "        print('epoch {} iter {} loss={}'.format(epoch, iter, loss.data.cpu().numpy()))\n",
    "\n",
    "    # Step the Learning Rate Scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "# ------------------------------------------ EVALUATION ------------------------------------------\n",
    "    # Perform Evaluation if Enabled & Current Epoch > 100\n",
    "    if evl_each and epoch > 100:\n",
    "        # Set Model to Evaluation Mode (Disables Dropout, etc.)\n",
    "        segNet.eval()  \n",
    "\n",
    "        # Generate Predictions & Save Grayscale + Colourised Outputs\n",
    "        gray_folder, color_folder = get_predictions(segNet, dataFolder, device)\n",
    "        \n",
    "        # Switch Back to Training Mode for Next Epoch\n",
    "        segNet.train()  \n",
    "\n",
    "        # Calculate mIoU for this Epoch\n",
    "        temp_mIoU = cal_acc(gray_folder, os.path.join(dataFolder, 'testing/label'))\n",
    "        IoU_list.append(temp_mIoU)\n",
    "\n",
    "        # Save Model if mIoU has Improved\n",
    "        if temp_mIoU > mIoU:\n",
    "            # Update Best mIoU Score\n",
    "            mIoU = temp_mIoU  \n",
    "\n",
    "            # Save Model\n",
    "            torch.save(segNet.state_dict(), './Models/OfficalModel.pth')  \n",
    "\n",
    "            # Move the Temporary Prediction Results to Final Output Folders\n",
    "            move_folders(gray_folder, color_folder, \n",
    "                         gray_folder.replace('temp_', ''), \n",
    "                         color_folder.replace('temp_', ''))\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# The Final mIoU is ~0.28\n",
    "print('The final mIoU is : {:.4f}.'.format(mIoU)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1747835847459,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "sM4LDbAO3OzL",
    "outputId": "b2f561f1-95e7-46c1-b983-23452360cab5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a Plot for the Data\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "\n",
    "# Add Title & Axis Labels\n",
    "plt.title(\"Training Loss Over Iterations\", fontweight = \"bold\", fontsize=\"16\")\n",
    "plt.xlabel(\"Training Iterations\", fontweight = \"bold\")\n",
    "plt.ylabel(\"Loss\", fontweight = \"bold\")\n",
    "\n",
    "# Add Grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Show Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1747835847518,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "1jBwoTvq3r7d",
    "outputId": "71632fe5-ac7e-46e8-df5b-d98ee21c3337"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (99,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m epochs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[32m101\u001b[39m, \u001b[32m200\u001b[39m))  \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Plot IoU vs. Epoch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIoU_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mIoU\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Add Title & Axis Labels\u001b[39;00m\n\u001b[32m      8\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mIoU Over Epochs\u001b[39m\u001b[33m\"\u001b[39m, fontweight = \u001b[33m\"\u001b[39m\u001b[33mbold\u001b[39m\u001b[33m\"\u001b[39m, fontsize=\u001b[33m\"\u001b[39m\u001b[33m16\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025/COMPUTER-VISION/.venv/lib/python3.12/site-packages/matplotlib/pyplot.py:3838\u001b[39m, in \u001b[36mplot\u001b[39m\u001b[34m(scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   3830\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.plot)\n\u001b[32m   3831\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot\u001b[39m(\n\u001b[32m   3832\u001b[39m     *args: \u001b[38;5;28mfloat\u001b[39m | ArrayLike | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3836\u001b[39m     **kwargs,\n\u001b[32m   3837\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[32m-> \u001b[39m\u001b[32m3838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3839\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3842\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3843\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3844\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025/COMPUTER-VISION/.venv/lib/python3.12/site-packages/matplotlib/axes/_axes.py:1777\u001b[39m, in \u001b[36mAxes.plot\u001b[39m\u001b[34m(self, scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1534\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1535\u001b[39m \u001b[33;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[32m   1536\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1774\u001b[39m \u001b[33;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1776\u001b[39m kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n\u001b[32m-> \u001b[39m\u001b[32m1777\u001b[39m lines = [*\u001b[38;5;28mself\u001b[39m._get_lines(\u001b[38;5;28mself\u001b[39m, *args, data=data, **kwargs)]\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[32m   1779\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_line(line)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025/COMPUTER-VISION/.venv/lib/python3.12/site-packages/matplotlib/axes/_base.py:297\u001b[39m, in \u001b[36m_process_plot_var_args.__call__\u001b[39m\u001b[34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m     this += args[\u001b[32m0\u001b[39m],\n\u001b[32m    296\u001b[39m     args = args[\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/2025/COMPUTER-VISION/.venv/lib/python3.12/site-packages/matplotlib/axes/_base.py:494\u001b[39m, in \u001b[36m_process_plot_var_args._plot_args\u001b[39m\u001b[34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[39m\n\u001b[32m    491\u001b[39m     axes.yaxis.update_units(y)\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.shape[\u001b[32m0\u001b[39m] != y.shape[\u001b[32m0\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx and y must have same first dimension, but \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.ndim > \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y.ndim > \u001b[32m2\u001b[39m:\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx and y can be no greater than 2D, but have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    498\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: x and y must have same first dimension, but have shapes (99,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Epochs from 101 to 350\n",
    "epochs = list(range(101, 200))  \n",
    "\n",
    "# Plot IoU vs. Epoch\n",
    "plt.plot(epochs, IoU_list, label=\"IoU\")\n",
    "\n",
    "# Add Title & Axis Labels\n",
    "plt.title(\"IoU Over Epochs\", fontweight = \"bold\", fontsize=\"16\")\n",
    "plt.xlabel(\"Epoch\", fontweight = \"bold\")\n",
    "plt.ylabel(\"IoU\", fontweight = \"bold\")\n",
    "\n",
    "# Add Grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Show Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnJBjS-Hli40"
   },
   "source": [
    "\n",
    "**Randomised Prediction Test Results from the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_predictions(temp_colour_folder, train_image_folder, num_images=3):\n",
    "    \"\"\"\n",
    "    Picks a few random predicted segmentation mask images and their corresponding original images,\n",
    "    then displays them side-by-side for visual comparison.\n",
    "    \n",
    "    Args:\n",
    "        temp_color_folder (str): Path to folder containing predicted color masks.\n",
    "        train_image_folder (str): Path to folder containing original training images.\n",
    "        num_images (int): Number of random samples to display.\n",
    "    \"\"\"\n",
    "\n",
    "    # List All Predicted Colour Mask Image Filenames\n",
    "    pred_images = os.listdir(temp_colour_folder)\n",
    "    \n",
    "    # Randomly Select 'num_images' Filenames From the Predictions List\n",
    "    selected_imgs = random.sample(pred_images, num_images)\n",
    "    \n",
    "    # Set Up Matplotlib Figure with Enough Height to Show All Selected Images Clearly\n",
    "    plt.figure(figsize=(15, num_images * 5))\n",
    "\n",
    "    # Loop Over the Selected Image Names & Plot Each Original & Predicted Pair\n",
    "    for i, img_name in enumerate(selected_imgs):\n",
    "        # Load Predicted Colour Mask Image (OpenCV Loads in BGR by Default)\n",
    "        pred_img = cv2.imread(os.path.join(temp_colour_folder, img_name))\n",
    "\n",
    "        # Convert Predicted Mask Image From BGR to RGB for Proper Colour Display in matplotlib\n",
    "        pred_img = cv2.cvtColor(pred_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load the Corresponding Original Training Image (also BGR)\n",
    "        orig_img = cv2.imread(os.path.join(train_image_folder, img_name))\n",
    "\n",
    "        # Convert Original Image from BGR to RGB for Display\n",
    "        orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Plot Original Image in Left Column\n",
    "        plt.subplot(num_images, 2, 2*i + 1)\n",
    "        plt.imshow(orig_img)\n",
    "        plt.title(f'Original Image: {img_name}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot Predicted Mask in Right Column\n",
    "        plt.subplot(num_images, 2, 2*i + 2)\n",
    "        plt.imshow(pred_img)\n",
    "        plt.title(f'Predicted Mask: {img_name}')\n",
    "        plt.axis('off') \n",
    "\n",
    "    # Adjust Spacing So Titles & Images Don't Overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the Plot Window\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './TempFiles/color'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m train_image_folder = \u001b[33m'\u001b[39m\u001b[33m./seg_data/testing/image\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Call the Function to Display 3 Random Prediction Results\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mshow_random_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_color_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_image_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_images\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mshow_random_predictions\u001b[39m\u001b[34m(temp_colour_folder, train_image_folder, num_images)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mPicks a few random predicted segmentation mask images and their corresponding original images,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mthen displays them side-by-side for visual comparison.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[33;03m    num_images (int): Number of random samples to display.\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# List All Predicted Colour Mask Image Filenames\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m pred_images = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_colour_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Randomly Select 'num_images' Filenames From the Predictions List\u001b[39;00m\n\u001b[32m     16\u001b[39m selected_imgs = random.sample(pred_images, num_images)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './TempFiles/color'"
     ]
    }
   ],
   "source": [
    "# Define Folder Paths for Predicted Masks & Original Images\n",
    "temp_color_folder = './TempFiles/color'\n",
    "train_image_folder = './seg_data/testing/image'\n",
    "\n",
    "# Call the Function to Display 3 Random Prediction Results\n",
    "show_random_predictions(temp_color_folder, train_image_folder, num_images=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FlshC7yryJZ"
   },
   "source": [
    "<br/><br>\n",
    "\n",
    "# **FLOPs**\n",
    "\n",
    "In deep learning, FLOPs *(Floating Point Operations)* quantify the total number of arithmetic operations—such as additions, multiplications, and divisions—that a model performs during a single forward pass *(i.e. when making a prediction)*. This metric serves as an indicator of a model’s computational complexity. When discussing large-scale models, FLOPs are often expressed in GFLOPs *(Giga Floating Point Operations)*, where 1 GFLOP equals one billion operations. This unit helps in comparing the computational demands of different models.\n",
    "\n",
    "**DO NOT MODIFT THIS CODE!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4944,
     "status": "ok",
     "timestamp": 1747835852463,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "C_D3Xsvu467x",
    "outputId": "df0bf371-a6e5-4b59-a25e-b2adf30e016f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fvcore in ./.venv/lib/python3.12/site-packages (0.1.5.post20221221)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from fvcore) (2.2.5)\n",
      "Requirement already satisfied: yacs>=0.1.6 in ./.venv/lib/python3.12/site-packages (from fvcore) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from fvcore) (6.0.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from fvcore) (4.67.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in ./.venv/lib/python3.12/site-packages (from fvcore) (3.1.0)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.12/site-packages (from fvcore) (11.2.1)\n",
      "Requirement already satisfied: tabulate in ./.venv/lib/python3.12/site-packages (from fvcore) (0.9.0)\n",
      "Requirement already satisfied: iopath>=0.1.7 in ./.venv/lib/python3.12/site-packages (from fvcore) (0.1.10)\n",
      "Requirement already satisfied: typing_extensions in ./.venv/lib/python3.12/site-packages (from iopath>=0.1.7->fvcore) (4.13.2)\n",
      "Requirement already satisfied: portalocker in ./.venv/lib/python3.12/site-packages (from iopath>=0.1.7->fvcore) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "# We use Fvcore to Calculate the FLOPs\n",
    "!pip3 install fvcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3669,
     "status": "ok",
     "timestamp": 1747835856134,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "J5fUBat44-hT",
    "outputId": "3d21678e-e92b-4028-ba64-4177526ee771"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::pad encountered 10 time(s)\n",
      "Unsupported operator aten::silu encountered 48 time(s)\n",
      "Unsupported operator aten::sigmoid encountered 16 time(s)\n",
      "Unsupported operator aten::mul encountered 16 time(s)\n",
      "Unsupported operator aten::add encountered 9 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder._avg_pooling, encoder._bn1, encoder._conv_head, encoder._dropout, segmentation_head.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 10.75 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "# Modifying the Size (3, 375, 1242) is ***NOT*** Allowed\n",
    "input = torch.randn(1, 3, 384, 1248)\n",
    "\n",
    "# Get the Network & its FLOPs\n",
    "model = smp.DeepLabV3Plus(encoder_name=\"efficientnet-b0\", Encoder_weights=\"imagenet\", in_channels=3, classes=19)\n",
    "model.eval()\n",
    "flops = FlopCountAnalysis(model, input)\n",
    "print(f\"FLOPs: {flops.total()/1e9:.2f} GFLOPs\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
