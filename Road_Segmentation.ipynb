{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVgOlzfMdiHe"
   },
   "source": [
    "<h1 style=\"font-size: 32px; font-weight: bold;\">Semantic Segmentation Competition (30%)</h1>\n",
    "\n",
    "For this competition, we will use a small autonomous driving dataset. The dataset contains 150 training images and 50 testing images.\n",
    "\n",
    "**We provide baseline code that includes the following features:**\n",
    "\n",
    "*    Loading the dataset using PyTorch.\n",
    "*    Defining a simple convolutional neural network for semantic segmentation.\n",
    "*    How to use existing loss function for the model learning.\n",
    "*    Train the network on the training data.\n",
    "*    Test the trained network on the testing data.\n",
    "<br/><br/>\n",
    "\n",
    "**The following changes could be considered:**\n",
    "\n",
    "1. Data augmentation\n",
    "2. Change of advanced training parameters: Learning Rate, Optimizer, Batch-size, and Drop-out.\n",
    "3. Architectural changes: Batch Normalization, Residual layers, etc.\n",
    "4. Use of a new loss function.\n",
    "\n",
    "Your code should be modified from the provided baseline. A pdf report of a maximum of two pages is required to explain the changes you made from the baseline, why you chose those changes, and the improvements they achieved.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "**Marking Rules:**\n",
    "\n",
    "We will mark the competition based on the final test accuracy on testing images and your report.\n",
    "\n",
    "Final mark (out of 50) = accuracy mark + efficiency mark + report mark\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "***Accuracy Mark 10:***\n",
    "\n",
    "We will rank all the submission results based on their test accuracy. Zero improvement over the baseline yields 0 marks. Maximum improvement over the baseline will yield 10 marks. There will be a sliding scale applied in between.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "***Efficiency Mark 10:***\n",
    "\n",
    "Efficiency considers not only the accuracy, but the computational cost of running the model (flops: https://en.wikipedia.org/wiki/FLOPS). Efficiency for our purposes is defined to be the ratio of accuracy (in %) to Gflops. Please report the computational cost for your final model and include the efficiency calculation in your report. Maximum improvement over the baseline will yield 10 marks. Zero improvement over the baseline yields zero marks, with a sliding scale in between.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "***Report mark 30:***\n",
    "\n",
    "**Your report should comprise:**\n",
    "\n",
    "1. An introduction showing your understanding of the task and of the baseline model: [10 marks]\n",
    "\n",
    "2. A description of how you have modified aspects of the system to improve performance. [10 marks]\n",
    "\n",
    "> A recommended way to present a summary of this is via an \"ablation study\" table, eg:\n",
    "\n",
    "> |Method1|Method2|Method3|Accuracy|\n",
    "> |---|---|---|---|\n",
    "> |N|N|N|60%|\n",
    "> |Y|N|N|65%|\n",
    "> |Y|Y|N|77%|\n",
    "> |Y|Y|Y|82%|\n",
    "\n",
    "3. Explanation of the methods for reducing the computational cost and/or improve the trade-off between accuracy and cost: [5 marks]\n",
    "\n",
    "4. Limitations/Conclusions: [5 marks]\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "**THIS REPORT CONTAINS THE OUTPUT FOR THE FINAL & BEST PERFORMING MODEL**\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0dtUDczT_fB"
   },
   "source": [
    "# **Download Data & Set Configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747833719681,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "XCPZsWI_9s-Y"
   },
   "outputs": [],
   "source": [
    "##################################################################################################################################\n",
    "### Subject: Computer Vision\n",
    "### Year: 2025\n",
    "### Student Name: Max Busato, Liam Hennig\n",
    "### Student ID: a1851532, a1852130\n",
    "### Comptetion Name: Semantic Segmentation Competition\n",
    "### Final Results:\n",
    "### ACC:         GFLOPs: 20.32 or 84.03\n",
    "##################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747833719692,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "xHPzdgeP67Xu"
   },
   "source": [
    "## **Download the Dataset**\n",
    "\n",
    "1. Dowanload and unzip the dataset. \n",
    "\n",
    "2. Run the code below to download and correctly store the pretrained model weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Library\n",
    "import os\n",
    "\n",
    "# Make Directory to Store Model Weights\n",
    "directory_name = \"Models\"\n",
    "\n",
    "# Create the Directory\n",
    "try:\n",
    "    os.mkdir(directory_name)\n",
    "    print(f\"Directory '{directory_name}' created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Directory '{directory_name}' already exists.\")\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied: Unable to create '{directory_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Download All the Weights from Google Drive\n",
    "FILE_ID=\"1NxYmFAJBMh3l2u-yNBBPf9zAIsFktfpH\"\n",
    "FILE_NAME=\"OfficalModel.pth\"\n",
    "!gdown --id $FILE_ID -O Models/$FILE_NAME\n",
    "\n",
    "FILE_ID=\"1UuB_nyVTNiRJ5NGKFB0FYx-upR9Vlzzg\"\n",
    "FILE_NAME=\"PreTrained_Seg_B3.pth\"\n",
    "!gdown --id $FILE_ID -O Models/$FILE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "## **Set Configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8909,
     "status": "ok",
     "timestamp": 1747833728595,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "106OzI7yUPON",
    "outputId": "84e98f0a-f948-4a7b-cd0d-aaa89e5d5570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hal-9000/Documents/Computer-Vision/.venv/lib/python3.10/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.7'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ Standard Library ------------------------\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "# ---------------------- External Dependencies ---------------------\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# --------------------------- PyTorch Core -------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ------------------------ PyTorch Ecosystem -----------------------\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as tf\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchinfo import summary\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# ------------------------ Data Augmentation -----------------------\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "#-------------------------------- Data path -----------------------------\n",
    "# Use your data path to replace the following path if you use Google drive.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Dataset Path. Ensure that the File Path Corresponds to the Path you Have Here. It is Expected that you Unzip the Data Folders before Running the Notebook.\n",
    "dataFolder = './seg_data'\n",
    "\n",
    "# To access Google Colab GPU; Go To: Edit >>> Notebook Settings >>> Hardware Accelarator: Select GPU.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## **Edit Configurations to Tune Model Performance**\n",
    "\n",
    "This section allows you to modify core training parameters such as the learning rate, batch size, image dimensions, and number of training epochs. These hyperparameters can significantly affect model performance and should be tuned based on your dataset and hardware constraints. The code also includes basic checks to ensure that the data path is valid and that CUDA is available for GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Editable Configuration Settings\n",
    "learning_rate = 1e-4 # Using a Learning Rate Scheduler\n",
    "width = 864\n",
    "height = 256 \n",
    "batchSize = 4 # Maxed-Out Due to Hardware Limitations\n",
    "epochs = 200  # Can Be Adjusted\n",
    "\n",
    "# Data Check\n",
    "if not os.path.exists(dataFolder):\n",
    "  print('Data Path Error! Please check your data path')\n",
    "\n",
    "# CUDA Check\n",
    "if not torch.cuda.is_available():\n",
    "  print('WARNING! The device is CPU NOT GPU! Please avoid using CPU for training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iM0C-LdtUThm"
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "# **Define a Dataloader to Load Data**\n",
    "\n",
    "This class handles the loading of images and their corresponding labels while applying systematic data transformations. Since the dataset is small, effective data augmentation such as flipping, resizing, rotation, brightness adjustments, and noise addition is used to improve model generalisation. The transformations ensure that both images and masks are consistently processed and converted into PyTorch tensors for training. The loader also includes sanity checks to confirm data integrity and correct alignment between images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1747833728629,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "_5dna2XkVHIq",
    "outputId": "ef7355a5-5c76-4bd4-8188-eb783007b805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded info for 150 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hal-9000/Documents/Computer-Vision/.venv/lib/python3.10/site-packages/albumentations/core/validation.py:111: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# The Class to Load Images & Labels\n",
    "class ExpDataSet(Dataset):\n",
    "    def __init__(self, dataFolder):\n",
    "        # Define the Paths to the Training Image & Label Directories\n",
    "        self.image_dir = os.path.join(dataFolder, \"training/image\")\n",
    "        self.label_dir = os.path.join(dataFolder, \"training/label\")\n",
    "\n",
    "        # List & Sort All Image & Label Filenames to Ensure Correct Alignment\n",
    "        self.image_path = sorted(os.listdir(self.image_dir))\n",
    "        self.label_path = sorted(os.listdir(self.label_dir))\n",
    "\n",
    "        # Print the Number of Samples Loaded\n",
    "        print(f'Loaded info for {len(self.image_path)} images')\n",
    "\n",
    "        # Ensure that the Number of Images is Exactly What We Expect (Sanity Check)\n",
    "        assert len(self.image_path) == 150\n",
    "\n",
    "        # Loop Through Each Index To:\n",
    "            # Check that Image & Label Filenames Match\n",
    "            # Store Full Paths (Including Directory) for Loading Later\n",
    "        for idx in range(len(self.image_path)):\n",
    "            assert self.image_path[idx] == self.label_path[idx], f\"Image and label filenames do not match at index {idx}\"\n",
    "            self.image_path[idx] = os.path.join(self.image_dir, self.image_path[idx])\n",
    "            self.label_path[idx] = os.path.join(self.label_dir, self.label_path[idx])\n",
    "            \n",
    "            \n",
    "        # -------------------------------- Transformation Functions --------------------------------\n",
    "        self.transform = A.Compose([\n",
    "            # Resize to Fixed Size\n",
    "            A.Resize(height, width, interpolation=cv2.INTER_NEAREST),\n",
    "\n",
    "            # Geometric Augmentations\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.1,\n",
    "                scale_limit=0.1,\n",
    "                rotate_limit=20,\n",
    "                border_mode=cv2.BORDER_CONSTANT,  # Use Constant Padding\n",
    "                p=0.4\n",
    "            ),\n",
    "\n",
    "            # Photometric Augmentations (Only On Images, Won't Affect Masks)\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.ColorJitter(p=0.3),\n",
    "            A.HueSaturationValue(p=0.3),\n",
    "\n",
    "            # Blurring/Noise\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(p=0.2), \n",
    "                A.GaussianBlur(p=0.2), \n",
    "                A.GaussNoise(p=0.2)\n",
    "            ], p=0.2),\n",
    "\n",
    "            # Normalisation\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                        std=(0.229, 0.224, 0.225)),\n",
    "\n",
    "            # Convert to PyTorch Tensor\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load Image & Label Using OpenCV\n",
    "        img = cv2.imread(self.image_path[idx])[:, :, ::-1]  # Convert BGR to RGB\n",
    "        label = cv2.imread(self.label_path[idx], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Apply Albumentations (With Nearest-Neighbour Mask Handling)\n",
    "        augmented = self.transform(image=img, mask=label)\n",
    "        img = augmented['image']           # Tensor: (3, H, W)\n",
    "        label = augmented['mask'].long()   # Tensor: (H, W), type: long\n",
    "\n",
    "        # Return the Image & Mask\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "\n",
    "# Get the Predefined Dataloader\n",
    "exp_data = ExpDataSet(dataFolder)\n",
    "train_loader = DataLoader(exp_data, batch_size=batchSize, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwfMTxpbVVSt"
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "# **Define a Convolutional Neural Network**\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a class of deep learning models specifically designed for processing data with a grid-like topology, such as images. They leverage convolutional layers to automatically and adaptively learn spatial hierarchies of features through backpropagation. Key components include:\n",
    "\n",
    "- **Convolutional Layers:** Apply learnable filters/kernels that slide across the input to produce feature maps, capturing spatial features like edges, textures, and shapes.\n",
    "\n",
    "- **Pooling Layers:** Downsample feature maps to reduce spatial dimensions and computation, while providing translation invariance.\n",
    "\n",
    "- **Activation Functions:** Non-linear functions *(e.g. ReLU)* applied after convolutions to introduce non-linearity, enabling the model to learn complex patterns.\n",
    "\n",
    "- **Fully Connected Layers:** Typically at the network’s end for classification tasks, mapping extracted features to output labels.\n",
    "\n",
    "- **Advantages:** CNNs reduce the number of parameters compared to fully connected networks by weight sharing and local connectivity, making them highly effective for image-related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## **Building a New CNN Model**\n",
    "\n",
    "BetterSegNetwork is a deep convolutional encoder-decoder architecture tailored for semantic segmentation, inspired by U-Net designs with residual connections and dilated convolutions. The model's structure and design choices include:\n",
    "\n",
    "- **Encoder:** Extracts hierarchical features with increasing channel depth. Uses residual blocks and dilated convolutions to expand receptive fields without losing resolution.\n",
    "\n",
    "- **Center Block:** Enhances contextual feature learning via increased dilation, capturing broader image context.\n",
    "\n",
    "- **Decoder:** Gradually upsamples and fuses encoder features through skip connections to reconstruct fine spatial details for pixel-level segmentation.\n",
    "\n",
    "- **Regularisation:** Batch normalisation and dropout are used throughout to improve training stability and reduce overfitting.\n",
    "\n",
    "- **Transfer Learning:** Pretrained weights are employed in the encoder to leverage learned features from large datasets, accelerating convergence and improving generalisation on limited data.\n",
    "\n",
    "- **Output:** Final layers generate per-pixel class probabilities for accurate segmentation maps.\n",
    "\n",
    "This combination of design elements balances model complexity with generalisation, making BetterSegNetwork suitable for challenging segmentation tasks, especially when training data is scarce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Block Using U-Net Style Concatenation\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels + skip_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.gn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.gn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        # Upsample x to Match Skip Spatial Size\n",
    "        x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Concatenate Along Channels\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.relu1(self.gn1(self.conv1(x)))\n",
    "        x = self.relu2(self.gn2(self.conv2(x)))\n",
    "        \n",
    "        # Return th Result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterSegNetwork(nn.Module):\n",
    "    def __init__(self, n_class=19):\n",
    "        super(BetterSegNetwork, self).__init__()\n",
    "\n",
    "        # ----------------------------- Build a Custom Encoder -------------------------------\n",
    "        # Stage 1\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.gn1_1 = nn.BatchNorm2d(64)\n",
    "        self.relu1_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.gn1_2 = nn.BatchNorm2d(64)\n",
    "        self.relu1_2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Pooling Layer 1/2 Scale\n",
    "        self.pool1 = nn.MaxPool2d(2, 2, ceil_mode=True) # --> 1/2\n",
    "\n",
    "        # Residual Conv for Stage 1 (input channels 3 → 64)\n",
    "        self.res_conv1 = nn.Conv2d(3, 64, kernel_size=1)  \n",
    "\n",
    "        # Stage 2\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.gn2_1 = nn.BatchNorm2d(128)\n",
    "        self.relu2_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.gn2_2 = nn.BatchNorm2d(128)\n",
    "        self.relu2_2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Pooling Layer 1/2 Scale\n",
    "        self.pool2 = nn.MaxPool2d(2, 2, ceil_mode=True) # --> 1/4\n",
    "\n",
    "        # Residual Conv for Stage 2 (input channels 64 → 128)\n",
    "        self.res_conv2 = nn.Conv2d(64, 128, kernel_size=1)\n",
    "\n",
    "        # Stage 3\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.gn3_1 = nn.BatchNorm2d(256)\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.gn3_2 = nn.BatchNorm2d(256)\n",
    "        self.relu3_2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.gn3_3 = nn.BatchNorm2d(256)\n",
    "        self.relu3_3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Pooling Layer 1/2 Scale\n",
    "        self.pool3 = nn.MaxPool2d(2, 2, ceil_mode=True) # --> 1/8\n",
    "\n",
    "        # Residual Conv for Stage 3 (input channels 128 → 256)\n",
    "        self.res_conv3 = nn.Conv2d(128, 256, kernel_size=1)\n",
    "\n",
    "        # Stage 4\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.gn4_1 = nn.BatchNorm2d(512)\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.drop4_1 = nn.Dropout2d(0.1)\n",
    "\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.gn4_2 = nn.BatchNorm2d(512)\n",
    "        self.relu4_2 = nn.ReLU(inplace=True)\n",
    "        self.drop4_2 = nn.Dropout2d(0.1)\n",
    "\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.gn4_3 = nn.BatchNorm2d(512)\n",
    "        self.relu4_3 = nn.ReLU(inplace=True)\n",
    "        self.drop4_3 = nn.Dropout2d(0.1)\n",
    "\n",
    "        # Pooling Layer 1/2 Scale\n",
    "        self.pool4 = nn.MaxPool2d(2, 2, ceil_mode=True) # --> 1/16\n",
    "\n",
    "        # Residual Conv for Stage 4 (input channels 256 → 512)\n",
    "        self.res_conv4 = nn.Conv2d(256, 512, kernel_size=1)\n",
    "\n",
    "        # Stage 5\n",
    "        self.conv5_1 = nn.Conv2d(512, 1024, 3, padding=2, dilation=2)  # Dilation = 2 expands receptive field\n",
    "        self.gn5_1 = nn.BatchNorm2d(1024)\n",
    "        self.relu5_1 = nn.ReLU(inplace=True)\n",
    "        self.drop5_1 = nn.Dropout2d(0.2)\n",
    "\n",
    "        self.conv5_2 = nn.Conv2d(1024, 1024, 3, padding=2, dilation=2)\n",
    "        self.gn5_2 = nn.BatchNorm2d(1024)\n",
    "        self.relu5_2 = nn.ReLU(inplace=True)\n",
    "        self.drop5_2 = nn.Dropout2d(0.2)\n",
    "\n",
    "        self.conv5_3 = nn.Conv2d(1024, 1024, 3, padding=2, dilation=2)\n",
    "        self.gn5_3 = nn.BatchNorm2d(1024)\n",
    "        self.relu5_3 = nn.ReLU(inplace=True)\n",
    "        self.drop5_3 = nn.Dropout2d(0.2)\n",
    "\n",
    "        # Pooling Layer 1/2 Scale\n",
    "        self.pool5 = nn.MaxPool2d(2, 2, ceil_mode=True) \n",
    "\n",
    "        # Residual Conv for Stage 5 (input channels 512 → 1024)\n",
    "        self.res_conv5 = nn.Conv2d(512, 1024, kernel_size=1)\n",
    "        # --------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        # Center Block (Switch Direction)\n",
    "        self.center_conv1 = nn.Conv2d(1024, 512, 3, padding=4, dilation=4)  # Bigger Dilation Here\n",
    "        self.center_gn1 = nn.BatchNorm2d(512)\n",
    "        self.center_relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.center_conv2 = nn.Conv2d(512, 512, 3, padding=4, dilation=4)\n",
    "        self.center_gn2 = nn.BatchNorm2d(512)\n",
    "        self.center_relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "        # ------------------------------- Build a Custom Decoder -----------------------------------------\n",
    "        self.dec5 = DecoderBlock(in_channels=512, skip_channels=512, out_channels=512)  # 512 + 512 -> 512\n",
    "        self.dec4 = DecoderBlock(in_channels=512, skip_channels=256, out_channels=256)  # 512 + 256 -> 256\n",
    "        self.dec3 = DecoderBlock(in_channels=256, skip_channels=128, out_channels=128)  # 256 + 128 -> 128\n",
    "        self.dec2 = DecoderBlock(in_channels=128, skip_channels=64, out_channels=64)    # 128 + 64 -> 64\n",
    "        # ------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        # Final Segmentation Head: Predict n_class Channels\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1),  # Helps Generalise with Small Datasets\n",
    "\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1),  # Helps Generalise with Small Datasets\n",
    "\n",
    "            nn.Conv2d(32, n_class, kernel_size=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stage 1\n",
    "        residual = self.res_conv1(x)\n",
    "        out = self.relu1_1(self.gn1_1(self.conv1_1(x)))\n",
    "        out = self.relu1_2(self.gn1_2(self.conv1_2(out)))\n",
    "        x = out + residual  # Residual Connection\n",
    "        c1 = self.pool1(x)  # --> 1/2\n",
    "\n",
    "        # Stage 2\n",
    "        residual = self.res_conv2(c1)\n",
    "        out = self.relu2_1(self.gn2_1(self.conv2_1(c1)))\n",
    "        out = self.relu2_2(self.gn2_2(self.conv2_2(out)))\n",
    "        x = out + residual  # Residual Connection\n",
    "        c2 = self.pool2(x)  # --> 1/4\n",
    "\n",
    "        # Stage 3\n",
    "        residual = self.res_conv3(c2)\n",
    "        out = self.relu3_1(self.gn3_1(self.conv3_1(c2)))\n",
    "        out = self.relu3_2(self.gn3_2(self.conv3_2(out)))\n",
    "        out = self.relu3_3(self.gn3_3(self.conv3_3(out)))\n",
    "        x = out + residual  # Residual Connection\n",
    "        c3 = self.pool3(x)  # --> 1/8\n",
    "\n",
    "        # Stage 4\n",
    "        residual = self.res_conv4(c3)\n",
    "        out = self.drop4_1(self.relu4_1(self.gn4_1(self.conv4_1(c3))))\n",
    "        out = self.drop4_2(self.relu4_2(self.gn4_2(self.conv4_2(out))))\n",
    "        out = self.drop4_3(self.relu4_3(self.gn4_3(self.conv4_3(out))))\n",
    "        x = out + residual  # Residual Connection\n",
    "        c4 = self.pool4(x)  # --> 1/16\n",
    "\n",
    "        # Stage 5\n",
    "        residual = self.res_conv5(c4)\n",
    "        out = self.drop5_1(self.relu5_1(self.gn5_1(self.conv5_1(c4))))\n",
    "        out = self.drop5_2(self.relu5_2(self.gn5_2(self.conv5_2(out))))\n",
    "        out = self.drop5_3(self.relu5_3(self.gn5_3(self.conv5_3(out))))\n",
    "        x = out + residual  # Residual Connection\n",
    "\n",
    "        # Center (Switch Direction)\n",
    "        x = self.center_relu1(self.center_gn1(self.center_conv1(x)))\n",
    "        x = self.center_relu2(self.center_gn2(self.center_conv2(x)))\n",
    "\n",
    "        # Decoder with Concatenation\n",
    "        x = self.dec5(x, c4)  # 1/16 scale\n",
    "        x = self.dec4(x, c3)  # 1/8 scale\n",
    "        x = self.dec3(x, c2)  # 1/4 scale\n",
    "        x = self.dec2(x, c1)  # 1/2 scale\n",
    "\n",
    "        # Final Upsample to Original Input Size (assumes input downscaled 1/2)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Final Segmentation Prediction\n",
    "        out = self.segmentation_head(x)  # (B, n_class, H, W)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained weights found at ./Models/pretrained_weights.pth, starting from scratch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BetterSegNetwork                         [1, 19, 256, 864]         --\n",
       "├─Conv2d: 1-1                            [1, 64, 256, 864]         256\n",
       "├─Conv2d: 1-2                            [1, 64, 256, 864]         1,792\n",
       "├─BatchNorm2d: 1-3                       [1, 64, 256, 864]         128\n",
       "├─ReLU: 1-4                              [1, 64, 256, 864]         --\n",
       "├─Conv2d: 1-5                            [1, 64, 256, 864]         36,928\n",
       "├─BatchNorm2d: 1-6                       [1, 64, 256, 864]         128\n",
       "├─ReLU: 1-7                              [1, 64, 256, 864]         --\n",
       "├─MaxPool2d: 1-8                         [1, 64, 128, 432]         --\n",
       "├─Conv2d: 1-9                            [1, 128, 128, 432]        8,320\n",
       "├─Conv2d: 1-10                           [1, 128, 128, 432]        73,856\n",
       "├─BatchNorm2d: 1-11                      [1, 128, 128, 432]        256\n",
       "├─ReLU: 1-12                             [1, 128, 128, 432]        --\n",
       "├─Conv2d: 1-13                           [1, 128, 128, 432]        147,584\n",
       "├─BatchNorm2d: 1-14                      [1, 128, 128, 432]        256\n",
       "├─ReLU: 1-15                             [1, 128, 128, 432]        --\n",
       "├─MaxPool2d: 1-16                        [1, 128, 64, 216]         --\n",
       "├─Conv2d: 1-17                           [1, 256, 64, 216]         33,024\n",
       "├─Conv2d: 1-18                           [1, 256, 64, 216]         295,168\n",
       "├─BatchNorm2d: 1-19                      [1, 256, 64, 216]         512\n",
       "├─ReLU: 1-20                             [1, 256, 64, 216]         --\n",
       "├─Conv2d: 1-21                           [1, 256, 64, 216]         590,080\n",
       "├─BatchNorm2d: 1-22                      [1, 256, 64, 216]         512\n",
       "├─ReLU: 1-23                             [1, 256, 64, 216]         --\n",
       "├─Conv2d: 1-24                           [1, 256, 64, 216]         590,080\n",
       "├─BatchNorm2d: 1-25                      [1, 256, 64, 216]         512\n",
       "├─ReLU: 1-26                             [1, 256, 64, 216]         --\n",
       "├─MaxPool2d: 1-27                        [1, 256, 32, 108]         --\n",
       "├─Conv2d: 1-28                           [1, 512, 32, 108]         131,584\n",
       "├─Conv2d: 1-29                           [1, 512, 32, 108]         1,180,160\n",
       "├─BatchNorm2d: 1-30                      [1, 512, 32, 108]         1,024\n",
       "├─ReLU: 1-31                             [1, 512, 32, 108]         --\n",
       "├─Dropout2d: 1-32                        [1, 512, 32, 108]         --\n",
       "├─Conv2d: 1-33                           [1, 512, 32, 108]         2,359,808\n",
       "├─BatchNorm2d: 1-34                      [1, 512, 32, 108]         1,024\n",
       "├─ReLU: 1-35                             [1, 512, 32, 108]         --\n",
       "├─Dropout2d: 1-36                        [1, 512, 32, 108]         --\n",
       "├─Conv2d: 1-37                           [1, 512, 32, 108]         2,359,808\n",
       "├─BatchNorm2d: 1-38                      [1, 512, 32, 108]         1,024\n",
       "├─ReLU: 1-39                             [1, 512, 32, 108]         --\n",
       "├─Dropout2d: 1-40                        [1, 512, 32, 108]         --\n",
       "├─MaxPool2d: 1-41                        [1, 512, 16, 54]          --\n",
       "├─Conv2d: 1-42                           [1, 1024, 16, 54]         525,312\n",
       "├─Conv2d: 1-43                           [1, 1024, 16, 54]         4,719,616\n",
       "├─BatchNorm2d: 1-44                      [1, 1024, 16, 54]         2,048\n",
       "├─ReLU: 1-45                             [1, 1024, 16, 54]         --\n",
       "├─Dropout2d: 1-46                        [1, 1024, 16, 54]         --\n",
       "├─Conv2d: 1-47                           [1, 1024, 16, 54]         9,438,208\n",
       "├─BatchNorm2d: 1-48                      [1, 1024, 16, 54]         2,048\n",
       "├─ReLU: 1-49                             [1, 1024, 16, 54]         --\n",
       "├─Dropout2d: 1-50                        [1, 1024, 16, 54]         --\n",
       "├─Conv2d: 1-51                           [1, 1024, 16, 54]         9,438,208\n",
       "├─BatchNorm2d: 1-52                      [1, 1024, 16, 54]         2,048\n",
       "├─ReLU: 1-53                             [1, 1024, 16, 54]         --\n",
       "├─Dropout2d: 1-54                        [1, 1024, 16, 54]         --\n",
       "├─Conv2d: 1-55                           [1, 512, 16, 54]          4,719,104\n",
       "├─BatchNorm2d: 1-56                      [1, 512, 16, 54]          1,024\n",
       "├─ReLU: 1-57                             [1, 512, 16, 54]          --\n",
       "├─Conv2d: 1-58                           [1, 512, 16, 54]          2,359,808\n",
       "├─BatchNorm2d: 1-59                      [1, 512, 16, 54]          1,024\n",
       "├─ReLU: 1-60                             [1, 512, 16, 54]          --\n",
       "├─DecoderBlock: 1-61                     [1, 512, 16, 54]          --\n",
       "│    └─Conv2d: 2-1                       [1, 512, 16, 54]          4,719,104\n",
       "│    └─BatchNorm2d: 2-2                  [1, 512, 16, 54]          1,024\n",
       "│    └─ReLU: 2-3                         [1, 512, 16, 54]          --\n",
       "│    └─Conv2d: 2-4                       [1, 512, 16, 54]          2,359,808\n",
       "│    └─BatchNorm2d: 2-5                  [1, 512, 16, 54]          1,024\n",
       "│    └─ReLU: 2-6                         [1, 512, 16, 54]          --\n",
       "├─DecoderBlock: 1-62                     [1, 256, 32, 108]         --\n",
       "│    └─Conv2d: 2-7                       [1, 256, 32, 108]         1,769,728\n",
       "│    └─BatchNorm2d: 2-8                  [1, 256, 32, 108]         512\n",
       "│    └─ReLU: 2-9                         [1, 256, 32, 108]         --\n",
       "│    └─Conv2d: 2-10                      [1, 256, 32, 108]         590,080\n",
       "│    └─BatchNorm2d: 2-11                 [1, 256, 32, 108]         512\n",
       "│    └─ReLU: 2-12                        [1, 256, 32, 108]         --\n",
       "├─DecoderBlock: 1-63                     [1, 128, 64, 216]         --\n",
       "│    └─Conv2d: 2-13                      [1, 128, 64, 216]         442,496\n",
       "│    └─BatchNorm2d: 2-14                 [1, 128, 64, 216]         256\n",
       "│    └─ReLU: 2-15                        [1, 128, 64, 216]         --\n",
       "│    └─Conv2d: 2-16                      [1, 128, 64, 216]         147,584\n",
       "│    └─BatchNorm2d: 2-17                 [1, 128, 64, 216]         256\n",
       "│    └─ReLU: 2-18                        [1, 128, 64, 216]         --\n",
       "├─DecoderBlock: 1-64                     [1, 64, 128, 432]         --\n",
       "│    └─Conv2d: 2-19                      [1, 64, 128, 432]         110,656\n",
       "│    └─BatchNorm2d: 2-20                 [1, 64, 128, 432]         128\n",
       "│    └─ReLU: 2-21                        [1, 64, 128, 432]         --\n",
       "│    └─Conv2d: 2-22                      [1, 64, 128, 432]         36,928\n",
       "│    └─BatchNorm2d: 2-23                 [1, 64, 128, 432]         128\n",
       "│    └─ReLU: 2-24                        [1, 64, 128, 432]         --\n",
       "├─Sequential: 1-65                       [1, 19, 256, 864]         --\n",
       "│    └─Conv2d: 2-25                      [1, 64, 256, 864]         36,928\n",
       "│    └─BatchNorm2d: 2-26                 [1, 64, 256, 864]         128\n",
       "│    └─ReLU: 2-27                        [1, 64, 256, 864]         --\n",
       "│    └─Conv2d: 2-28                      [1, 64, 256, 864]         36,928\n",
       "│    └─BatchNorm2d: 2-29                 [1, 64, 256, 864]         128\n",
       "│    └─ReLU: 2-30                        [1, 64, 256, 864]         --\n",
       "│    └─Dropout2d: 2-31                   [1, 64, 256, 864]         --\n",
       "│    └─Conv2d: 2-32                      [1, 32, 256, 864]         18,464\n",
       "│    └─BatchNorm2d: 2-33                 [1, 32, 256, 864]         64\n",
       "│    └─ReLU: 2-34                        [1, 32, 256, 864]         --\n",
       "│    └─Conv2d: 2-35                      [1, 32, 256, 864]         9,248\n",
       "│    └─BatchNorm2d: 2-36                 [1, 32, 256, 864]         64\n",
       "│    └─ReLU: 2-37                        [1, 32, 256, 864]         --\n",
       "│    └─Dropout2d: 2-38                   [1, 32, 256, 864]         --\n",
       "│    └─Conv2d: 2-39                      [1, 19, 256, 864]         627\n",
       "==========================================================================================\n",
       "Total params: 49,305,075\n",
       "Trainable params: 49,305,075\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 143.17\n",
       "==========================================================================================\n",
       "Input size (MB): 2.65\n",
       "Forward/backward pass size (MB): 2135.75\n",
       "Params size (MB): 197.22\n",
       "Estimated Total Size (MB): 2335.63\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Model Instance (must match the architecture exactly)\n",
    "segNet = BetterSegNetwork(n_class=19).to(device)\n",
    "\n",
    "# Try to Load the Pretrained Weights\n",
    "weights_path = './Models/pretrained_weights.pth'\n",
    "\n",
    "# Check that there are Weights Avaliable\n",
    "if os.path.isfile(weights_path):\n",
    "    print(f\"Loading pretrained weights from {weights_path}\")\n",
    "    segNet.load_state_dict(torch.load(weights_path))\n",
    "else:\n",
    "    print(f\"No pretrained weights found at {weights_path}, starting from scratch.\")\n",
    "\n",
    "# Print Model Summary With Input Size (batch_size, channels, height, width)\n",
    "summary(segNet, input_size=(1, 3, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## **Making a Pre-Trained Hybrid Model**\n",
    "\n",
    "This model architecture combines EfficientNet-B3 as the encoder backbone with SegFormer as the decoder for semantic segmentation:\n",
    "\n",
    "- **Encoder Backbone:** EfficientNet-B3 pretrained on ImageNet, providing efficient and robust feature extraction.\n",
    "\n",
    "- **Decoder:** SegFormer decoder, known for its efficient and accurate multi-scale feature aggregation without requiring positional encoding.\n",
    "\n",
    "- **Device Configuration:** Automatically selects GPU if available, otherwise runs on CPU.\n",
    "\n",
    "- **Model Initialisation:** Uses `segmentation_models_pytorch` (SMP) library’s SegFormer implementation with EfficientNet-B3 encoder and 19 output segmentation classes.\n",
    "\n",
    "- **Pretrained Weights Loading:** Loads pretrained ***cityscapes*** weights from a checkpoint if available on google drive, while handling possible 'module.' prefixes from distributed training.\n",
    "\n",
    "- **Model Summary:** Outputs a detailed summary of the model architecture for the specified input size.\n",
    "\n",
    "This hybrid approach leverages EfficientNet-B3’s strong feature extraction and SegFormer’s efficient transformer-based decoding, enabling high-quality segmentation with reduced computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained weights found at ./Models/PreTrained_Seg_B3.pth, starting from scratch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "Segformer                                     [1, 19, 256, 864]         --\n",
       "├─MixVisionTransformerEncoder: 1-1            [1, 3, 256, 864]          --\n",
       "│    └─OverlapPatchEmbed: 2-1                 [1, 64, 64, 216]          --\n",
       "│    │    └─Conv2d: 3-1                       [1, 64, 64, 216]          9,472\n",
       "│    │    └─LayerNorm: 3-2                    [1, 64, 64, 216]          128\n",
       "│    └─Sequential: 2-2                        [1, 64, 64, 216]          --\n",
       "│    │    └─Block: 3-3                        [1, 64, 64, 216]          314,880\n",
       "│    │    └─Block: 3-4                        [1, 64, 64, 216]          314,880\n",
       "│    │    └─Block: 3-5                        [1, 64, 64, 216]          314,880\n",
       "│    └─LayerNorm: 2-3                         [1, 64, 64, 216]          128\n",
       "│    └─OverlapPatchEmbed: 2-4                 [1, 128, 32, 108]         --\n",
       "│    │    └─Conv2d: 3-6                       [1, 128, 32, 108]         73,856\n",
       "│    │    └─LayerNorm: 3-7                    [1, 128, 32, 108]         256\n",
       "│    └─Sequential: 2-5                        [1, 128, 32, 108]         --\n",
       "│    │    └─Block: 3-8                        [1, 128, 32, 108]         465,920\n",
       "│    │    └─Block: 3-9                        [1, 128, 32, 108]         465,920\n",
       "│    │    └─Block: 3-10                       [1, 128, 32, 108]         465,920\n",
       "│    │    └─Block: 3-11                       [1, 128, 32, 108]         465,920\n",
       "│    └─LayerNorm: 2-6                         [1, 128, 32, 108]         256\n",
       "│    └─OverlapPatchEmbed: 2-7                 [1, 320, 16, 54]          --\n",
       "│    │    └─Conv2d: 3-12                      [1, 320, 16, 54]          368,960\n",
       "│    │    └─LayerNorm: 3-13                   [1, 320, 16, 54]          640\n",
       "│    └─Sequential: 2-8                        [1, 320, 16, 54]          --\n",
       "│    │    └─Block: 3-14                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-15                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-16                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-17                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-18                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-19                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-20                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-21                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-22                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-23                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-24                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-25                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-26                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-27                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-28                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-29                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-30                       [1, 320, 16, 54]          1,656,320\n",
       "│    │    └─Block: 3-31                       [1, 320, 16, 54]          1,656,320\n",
       "│    └─LayerNorm: 2-9                         [1, 320, 16, 54]          640\n",
       "│    └─OverlapPatchEmbed: 2-10                [1, 512, 8, 27]           --\n",
       "│    │    └─Conv2d: 3-32                      [1, 512, 8, 27]           1,475,072\n",
       "│    │    └─LayerNorm: 3-33                   [1, 512, 8, 27]           1,024\n",
       "│    └─Sequential: 2-11                       [1, 512, 8, 27]           --\n",
       "│    │    └─Block: 3-34                       [1, 512, 8, 27]           3,172,864\n",
       "│    │    └─Block: 3-35                       [1, 512, 8, 27]           3,172,864\n",
       "│    │    └─Block: 3-36                       [1, 512, 8, 27]           3,172,864\n",
       "│    └─LayerNorm: 2-12                        [1, 512, 8, 27]           1,024\n",
       "├─SegformerDecoder: 1-2                       [1, 256, 64, 216]         --\n",
       "│    └─ModuleList: 2-13                       --                        --\n",
       "│    │    └─MLP: 3-37                         [1, 256, 8, 27]           131,328\n",
       "│    │    └─MLP: 3-38                         [1, 256, 16, 54]          82,176\n",
       "│    │    └─MLP: 3-39                         [1, 256, 32, 108]         33,024\n",
       "│    │    └─MLP: 3-40                         [1, 256, 64, 216]         16,640\n",
       "│    └─Conv2dReLU: 2-14                       [1, 256, 64, 216]         --\n",
       "│    │    └─Conv2d: 3-41                      [1, 256, 64, 216]         262,144\n",
       "│    │    └─BatchNorm2d: 3-42                 [1, 256, 64, 216]         512\n",
       "│    │    └─ReLU: 3-43                        [1, 256, 64, 216]         --\n",
       "├─SegmentationHead: 1-3                       [1, 19, 256, 864]         --\n",
       "│    └─Conv2d: 2-15                           [1, 19, 64, 216]          4,883\n",
       "│    └─UpsamplingBilinear2d: 2-16             [1, 19, 256, 864]         --\n",
       "│    └─Activation: 2-17                       [1, 19, 256, 864]         --\n",
       "│    │    └─Identity: 3-44                    [1, 19, 256, 864]         --\n",
       "===============================================================================================\n",
       "Total params: 44,602,835\n",
       "Trainable params: 44,602,835\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 7.13\n",
       "===============================================================================================\n",
       "Input size (MB): 2.65\n",
       "Forward/backward pass size (MB): 1199.59\n",
       "Params size (MB): 178.41\n",
       "Estimated Total Size (MB): 1380.66\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Model Instance using EfficientNet-B3 + SegFormer\n",
    "segNet = smp.Segformer(\n",
    "    encoder_name=\"efficientnet-b3\",   # EfficientNet-B3 Backbone\n",
    "    encoder_weights=\"imagenet\",       # Pretrained on ImageNet\n",
    "    in_channels=3,                    # RGB Images\n",
    "    classes=19                        # Output Segmentation Classes\n",
    ").to(device)\n",
    "\n",
    "# Print Model Summary\n",
    "summary(segNet, input_size=(1, 3, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## **Making a Mix Transformer Segmentation Model**\n",
    "\n",
    "This model employs the official SegFormer-B3 architecture, fine-tuned on the Cityscapes dataset, leveraging a pure transformer-based encoder-decoder design for semantic segmentation. The model delivers state-of-the-art accuracy with efficient computation, ideal for urban scene parsing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SegFormer Model Fine-tuned on Cityscapes dataset\n",
    "\n",
    "\n",
    "# https://github.com/qubvel-org/segmentation_models.pytorch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print Model Summary\n",
    "summary(segNet, input_size=(1, 3, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "## **Loss & Optimisation**\n",
    "\n",
    "### **Weighted Cross Entropy Loss**\n",
    "\n",
    "Weighted Cross Entropy (WCE) is a modification of the standard Cross Entropy loss that introduces class-specific weighting factors to address the issue of class imbalance, which is common in real-world segmentation datasets. In typical urban scene datasets *(e.g. Cityscapes-like)*, certain classes such as *“road”* or *“building”* may dominate the pixel distribution, while others like *“traffic light”* or *“person”* may be underrepresented. This imbalance can lead the model to bias its predictions toward the majority classes. \n",
    "\n",
    "By assigning higher weights to minority classes, WCE increases the penalty for misclassifying them, thereby encouraging the model to learn more balanced representations. The weights are typically computed as the inverse frequency of each class or using a smoothed variant such as median frequency balancing.\n",
    "\n",
    "In this implementation, the WCE loss was applied over the softmax outputs of the model, with class weights provided as a fixed tensor based on label statistics from the training dataset. This approach maintains the interpretability and differentiability of standard Cross Entropy while improving performance on underrepresented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Class Weights: tensor([2.1798e-03, 1.7441e-02, 8.1970e-03, 6.9577e-02, 8.0158e-02, 4.8503e-02,\n",
      "        2.3043e-01, 1.4927e-01, 2.2231e-03, 7.0945e-03, 6.1944e-03, 6.9403e-01,\n",
      "        2.5023e+00, 1.1694e-02, 2.5927e-01, 9.7917e-01, 3.2672e+00, 9.5387e+00,\n",
      "        1.1264e+00])\n"
     ]
    }
   ],
   "source": [
    "def compute_class_weights(dataloader, num_classes):\n",
    "    # Initialise an Array to Count Pixels for Each Class, Starting at Zero\n",
    "    class_counts = np.zeros(num_classes)\n",
    "\n",
    "    # Counter for Total Number of Valid Pixels Across Dataset\n",
    "    total_pixels = 0 \n",
    "\n",
    "    # Loop Through All Batches in the Dataloader\n",
    "    for _, masks in dataloader:\n",
    "        # Masks Shape: (B, H, W)\n",
    "        masks = masks.numpy()\n",
    "        \n",
    "        # Flatten Batch & Spatial Dims: (B * H * W)\n",
    "        masks_flat = masks.flatten()\n",
    "        \n",
    "        # Filter Out Ignore Labels\n",
    "        masks_flat = masks_flat[masks_flat != 255]\n",
    "        \n",
    "        # Count Pixels Per Class\n",
    "        counts = np.bincount(masks_flat, minlength=num_classes)\n",
    "        \n",
    "        # Accumulate Counts Per Class Over All Batches\n",
    "        class_counts += counts\n",
    "        \n",
    "        # Update Total Number of Valid Pixels Seen So Far\n",
    "        total_pixels += len(masks_flat)\n",
    "    \n",
    "    # Calculate Frequency Per Class\n",
    "    class_freqs = class_counts / total_pixels\n",
    "    \n",
    "    # Inverse Frequency Weights (Add Small Epsilon to Avoid Division by Zero)\n",
    "    class_weights = 1.0 / (class_freqs + 1e-6)\n",
    "    \n",
    "    # Normalise Weights So Their Sum Equals num_classes\n",
    "    class_weights = class_weights / class_weights.sum() * num_classes\n",
    "    \n",
    "    # Return the Weights\n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Calculate the Weights for CE Loss\n",
    "num_classes = 19\n",
    "weights = compute_class_weights(train_loader, num_classes)\n",
    "\n",
    "# Print the Computed Weights to Verify\n",
    "print(\"Computed Class Weights:\", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### **Multi-Class Dice Loss**\n",
    "\n",
    "To address the challenges of class imbalance and to better reflect the spatial structure of predicted segments, a custom Multi-Class Dice Loss function was implemented. Unlike standard Cross Entropy, which penalises misclassifications on a per-pixel basis, Dice Loss directly optimises for region-level similarity between the predicted and ground truth masks. This is particularly useful in semantic segmentation tasks where smaller or less frequent classes may be overwhelmed by dominant ones.\n",
    "\n",
    "The implemented loss extends the standard Dice formulation to multi-class settings by calculating the Dice coefficient (or F-score) per class after applying a softmax over class logits. Ground truth labels are one-hot encoded and aligned with the prediction tensor, while ignore labels (255) are excluded from the calculation. A smoothing term (epsilon) is included for numerical stability, and the formula is parameterised by a Beta² term to allow future adjustment of precision-recall trade-offs. In this instance, Beta was set to 1.0, placing equal emphasis on both precision and recall. The final loss is computed as the sum of Dice errors across all valid classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassDiceLoss(nn.Module):\n",
    "    def __init__(self, num_classes: int, eps: float = 1e-7, beta: float = 1.0, reduction: str = \"sum\"):\n",
    "        super(MulticlassDiceLoss, self).__init__()\n",
    "        # Select Device: GPU if Available Else CPU\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Create a Tensor of Ones with Length = Num_classes, Used for Loss Calculation\n",
    "        self.ones_tensor: torch.Tensor = torch.ones(num_classes, device=self.device)\n",
    "        \n",
    "        # Sum of Ones Tensor, Effectively the Number of Classes as a Float Tensor\n",
    "        self.num_classes = torch.sum(self.ones_tensor)\n",
    "        \n",
    "        # Store Integer Number of Classes for Use in One-Hot Encoding & Masking\n",
    "        self.num_classes_int = num_classes\n",
    "        \n",
    "        # Store the Reduction Method in Lowercase for Later Use\n",
    "        self.reduction: str = reduction.lower()\n",
    "        \n",
    "        # Small Epsilon to Prevent Division by Zero in Calculations\n",
    "        self.eps: float = eps\n",
    "        \n",
    "        # Square of Beta, Used in F-score Calculation to Weigh Precision Recall\n",
    "        self.beta2: float = beta ** 2\n",
    "\n",
    "\n",
    "    def multiclass_f_score(self, gt: torch.Tensor, pr: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply Softmax to Predictions to Get Class Probabilities Along Channel Dimension\n",
    "        pr_softmax = torch.softmax(pr, dim=1)  # Shape: (B, C, H, W)\n",
    "\n",
    "        # Replace Ignore Label (255) in Ground Truth With num_classes (to exclude in one-hot)\n",
    "        gt = gt.clone()\n",
    "        gt[gt == 255] = self.num_classes_int\n",
    "\n",
    "        # Convert Ground Truth to One-hot Encoding Including an Extra Class for Ignored Pixels\n",
    "        gt_one_hot = torch.nn.functional.one_hot(gt, num_classes=self.num_classes_int + 1)  # (B, H, W, C+1)\n",
    "        \n",
    "        # Exclude the Ignored Class by Slicing\n",
    "        gt_one_hot = gt_one_hot[:, :, :, :self.num_classes_int].float()\n",
    "        \n",
    "        # Change Shape from (B, H, W, C) to (B, C, H, W) to Align with Predictions\n",
    "        gt_one_hot = gt_one_hot.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "\n",
    "        # True Positives: Sum Over Batch & Spatial Dims where Prediction & GT Agree\n",
    "        tp = torch.sum(gt_one_hot * pr_softmax, dim=(0, 2, 3))\n",
    "        \n",
    "        # False Positives: Predicted Positives Minus True Positives\n",
    "        fp = torch.sum(pr_softmax, dim=(0, 2, 3)) - tp\n",
    "        \n",
    "        # False Negatives: Actual Positives Minus True Positives\n",
    "        fn = torch.sum(gt_one_hot, dim=(0, 2, 3)) - tp\n",
    "\n",
    "        # Calculate F-score with Smoothing (eps) & Beta Weighting\n",
    "        return ((1 + self.beta2) * tp + self.eps) / ((1 + self.beta2) * tp + self.beta2 * fn + fp + self.eps)\n",
    "\n",
    "\n",
    "    def forward(self, pr: torch.Tensor, gt: torch.Tensor) -> torch.Tensor:\n",
    "        # Calculate F-score for Each Class\n",
    "        f_score = self.multiclass_f_score(pr=pr, gt=gt)\n",
    "        \n",
    "        # Dice Loss is 1 - F-score\n",
    "        loss = self.ones_tensor.to(f_score.device) - f_score\n",
    "\n",
    "        # Apply Specified Reduction to the Loss Tensor\n",
    "        if self.reduction == \"none\":\n",
    "            # No Reduction, Return Per-Class Loss\n",
    "            pass\n",
    "        elif self.reduction == \"mean\":\n",
    "            # Mean Loss Over All Classes\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            # Sum Loss Over All Classes, Cast to float32 for Numerical Stability\n",
    "            loss = loss.sum(dtype=torch.float32)\n",
    "        elif self.reduction == \"batchwise_mean\":\n",
    "            # Sum Over Batch Dimension & Average Per Class (if Loss is Batchwise)\n",
    "            loss = loss.sum(dim=0, dtype=torch.float32)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### **Dice Loss With Cross Entropy**\n",
    "\n",
    "This combined loss function was used to balance pixel-wise classification accuracy with region-level overlap. Cross Entropy Loss focuses on penalising incorrect class predictions at the pixel level, while Dice Loss directly optimises for the similarity between the predicted and ground truth regions. By combining the two with equal weighting (0.5 each), the model was encouraged to perform well both at the granular pixel scale and in capturing the overall structure of objects. This approach was particularly beneficial in handling class imbalance and improving segmentation performance on smaller or less frequent classes, which may be underrepresented in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComboLoss(nn.Module):\n",
    "    def __init__(self, weight=0.5):\n",
    "        super().__init__()\n",
    "        # Get Cross Entropy Loss\n",
    "        self.ce = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "        # Get Dice Loss\n",
    "        self.dice = MulticlassDiceLoss(num_classes=19)\n",
    "\n",
    "        # Weight the Combination of the Two Losses\n",
    "        self.weight = weight\n",
    "\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = self.ce(logits, targets)\n",
    "        dice_loss = self.dice(logits, targets)\n",
    "        dice_loss_scaled = dice_loss / 19  # Scale Dice Loss Down to ~[0,1]\n",
    "        return self.weight * ce_loss + (1 - self.weight) * dice_loss_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### **Implementation of Chosen Loss Function & Optimiser**\n",
    "\n",
    "The segmentation model was trained using a combination of a carefully selected loss function and an appropriate optimiser to ensure stable convergence and effective learning. The loss function used was a combination of Multiclass Dice Loss and Cross Entropy Loss, chosen to address class imbalance and improve the overlap between predicted and ground truth segmentation regions. Dice Loss emphasises region-level accuracy by directly optimising for overlap, while Cross Entropy provides stable pixel-wise classification gradients. For optimisation, the AdamW optimiser was applied, with a learning rate of 1e-4 *(decreased to 1e-6)* and weight decay to promote generalisation. Additionally, a Cosine Annealing learning rate scheduler was employed to gradually reduce the learning rate, allowing the model to make large updates early in training and finer adjustments as it converges. The choice of optimiser, scheduler, and hyperparameters was guided by the need to balance convergence speed with stable and generalisable learning throughout training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1747833729540,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "ubLuJmG7XXcV"
   },
   "outputs": [],
   "source": [
    "# Define the Loss Function\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "# criterion = nn.CrossEntropyLoss(weight=weights.to(device), ignore_index=255)\n",
    "# criterion = MulticlassDiceLoss(num_classes=19)\n",
    "criterion = ComboLoss()\n",
    "\n",
    "\n",
    "# Define the Optimiser\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=segNet.parameters(), \n",
    "    lr=learning_rate,      # Lower Learning Rate to Avoid Overfitting\n",
    "    weight_decay=1e-4      # Helps Prevent Overfitting (Regularisation)\n",
    ")\n",
    "\n",
    "\n",
    "# Define the Leanring Rate Scheduler\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=epochs, eta_min=1e-6)\n",
    "# scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer,\n",
    "#     mode='min',\n",
    "#     factor=0.5,\n",
    "#     patience=5,\n",
    "#     min_lr=1e-8\n",
    "# )   # Must Test on every epoch, change stepping function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzUEUtY6aXMG"
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "# **The Function Used to Compare the Precision**\n",
    "\n",
    "**DO NOT MODIFY THIS CODE!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747833729540,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "2rFg1PNNa3Ch"
   },
   "outputs": [],
   "source": [
    "def cal_acc(pred_folder, gt_folder, classes=19):\n",
    "    class AverageMeter(object):\n",
    "        def __init__(self):\n",
    "            self.reset()\n",
    "        def reset(self):\n",
    "            self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "        def update(self, val, n=1):\n",
    "            self.val = val\n",
    "            self.sum += val * n\n",
    "            self.count += n\n",
    "            self.avg = self.sum / self.count\n",
    "    def intersectionAndUnion(output, target, K, ignore_index=255):\n",
    "        assert (output.ndim in [1, 2, 3])\n",
    "        assert output.shape == target.shape\n",
    "        output = output.reshape(output.size).copy()\n",
    "        target = target.reshape(target.size)\n",
    "        output[np.where(target == ignore_index)[0]] = ignore_index\n",
    "        intersection = output[np.where(output == target)[0]]\n",
    "        area_intersection, _ = np.histogram(intersection, bins=np.arange(K + 1))\n",
    "        area_output, _ = np.histogram(output, bins=np.arange(K + 1))\n",
    "        area_target, _ = np.histogram(target, bins=np.arange(K + 1))\n",
    "        area_union = area_output + area_target - area_intersection\n",
    "        return area_intersection, area_union, area_target\n",
    "    data_list = os.listdir(gt_folder)\n",
    "    intersection_meter = AverageMeter()\n",
    "    union_meter = AverageMeter()\n",
    "    target_meter = AverageMeter()\n",
    "    for i, image_name in enumerate(data_list):\n",
    "        pred = cv2.imread(os.path.join(pred_folder, image_name), cv2.IMREAD_GRAYSCALE)\n",
    "        target = cv2.imread(os.path.join(gt_folder, image_name), cv2.IMREAD_GRAYSCALE)\n",
    "        intersection, union, target = intersectionAndUnion(pred, target, classes)\n",
    "        intersection_meter.update(intersection)\n",
    "        union_meter.update(union)\n",
    "        target_meter.update(target)\n",
    "    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n",
    "    mIoU = np.mean(iou_class)\n",
    "    print('Eval result: mIoU {:.4f}.'.format(mIoU))\n",
    "    return mIoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2dRJ8TgboP9"
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "# **Define Functions to Get & Save Predictions**\n",
    "\n",
    "This section contains utility functions to manage prediction outputs during model evaluation. It includes creating necessary folders, moving temporary prediction files to final result directories, and converting grayscale masks into colorised images for better visualisation. The main function runs the model on test images, saves both grayscale and color predictions, and prepares them for further analysis or display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747833729541,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "dBcRAC2AhLzS"
   },
   "outputs": [],
   "source": [
    "# Create a Directory if it Does Not Already Exist\n",
    "def make_folder(dir_name): \n",
    "    if not os.path.exists(dir_name):\n",
    "        # Create the Directory & Any Necessary Parent Directories\n",
    "        os.makedirs(dir_name)  \n",
    "\n",
    "\n",
    "# Move All Files From Temporary Prediction Folders to Result Folders & Remove Temporary Folders\n",
    "def move_folders(grey_temp, color_temp, grey_rs, color_rs):\n",
    "    # If Temporary Grayscale Prediction Folder Exists\n",
    "    if os.path.exists(grey_temp):\n",
    "        # Ensure Destination Folder Exists\n",
    "        make_folder(grey_rs)  \n",
    "\n",
    "        # Move Each File From Temp to Result Directory\n",
    "        for file in os.listdir(grey_temp):\n",
    "            shutil.move(os.path.join(grey_temp, file), os.path.join(grey_rs, file))\n",
    "\n",
    "        # Remove the Temp Folder if it Still Exists After Moving\n",
    "        if os.path.exists(grey_temp):\n",
    "            shutil.rmtree(grey_temp)\n",
    "\n",
    "    # Same Process for the Colour Prediction Folder\n",
    "    if os.path.exists(color_temp):\n",
    "        make_folder(color_rs)\n",
    "        for file in os.listdir(color_temp):\n",
    "            shutil.move(os.path.join(color_temp, file), os.path.join(color_rs, file))\n",
    "        if os.path.exists(color_temp):\n",
    "            shutil.rmtree(color_temp)\n",
    "\n",
    "\n",
    "# Convert a Grayscale Mask (With Class Indices) to a Colorised Image Using a Palette\n",
    "def colorize(gray, palette):\n",
    "    # Convert the Grayscale NumPy Array to a PIL Image in 'P' Mode (Palette-Based)\n",
    "    color = Image.fromarray(gray.astype(np.uint8)).convert('P')\n",
    "    \n",
    "    # Apply the Palette (A Flat List of RGB Triplets For Each Class Index)\n",
    "    color.putpalette(palette)\n",
    "\n",
    "    # Return the Colorised Image\n",
    "    return color\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------- PERFORM EVALUATION FOR A NETWORK & SAVE PREDICTION RESULTS -----------------------\n",
    "def get_predictions(segNet, dataFolder, device):\n",
    "    # Define Folders to Save Grayscale & Colour Predictions\n",
    "    gray_folder, color_folder, model_folder = './TempFiles/temp_grey', './TempFiles/temp_color', './Models'\n",
    "    \n",
    "    # List All Image Filenames in the Testing Images Folder\n",
    "    listImages = os.listdir(os.path.join(dataFolder, \"testing/image\"))\n",
    "    \n",
    "    # Define the Path to Ground Truth Label Folder\n",
    "    gt_folder = os.path.join(dataFolder, \"testing/label\")\n",
    "    \n",
    "    # Path to Colour Palette for Visualisation\n",
    "    colors_path  = os.path.join(dataFolder, \"colors.txt\")\n",
    "    print('Begin Testing')\n",
    "    \n",
    "    # Create Folders if They Don't Exist\n",
    "    make_folder(gray_folder)\n",
    "    make_folder(color_folder)\n",
    "    make_folder(model_folder)\n",
    "    \n",
    "    # Load Colours for Visualising Grayscale Masks as RGB\n",
    "    colors = np.loadtxt(colors_path).astype('uint8')\n",
    "\n",
    "    # Albumentations Transform with Resize + Normalise + Tensor\n",
    "    transformTest = A.Compose([\n",
    "        A.Resize(height, width, interpolation=cv2.INTER_LINEAR),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    # Make Sure Model is in Eval Mode\n",
    "    segNet.eval()  \n",
    "\n",
    "    # Iterate Over All Images to Predict\n",
    "    with torch.no_grad():\n",
    "        for img_name in listImages:\n",
    "            img_path = os.path.join(dataFolder, \"testing/image\", img_name)\n",
    "            img = cv2.imread(img_path)[:, :, ::-1]  # BGR to RGB\n",
    "            \n",
    "            # Apply Albumentations transform\n",
    "            augmented = transformTest(image=img)\n",
    "            img_tensor = augmented['image'].unsqueeze(0).to(device)  # (1,3,H,W)\n",
    "\n",
    "            prediction = segNet(img_tensor)[0].cpu().numpy()\n",
    "            prediction = np.argmax(prediction, axis=0)  # (H, W)\n",
    "\n",
    "            gt_mask = cv2.imread(os.path.join(gt_folder, img_name), cv2.IMREAD_GRAYSCALE)\n",
    "            gt_h, gt_w = gt_mask.shape\n",
    "\n",
    "            # Resize prediction to original GT size using nearest neighbor\n",
    "            prediction_resized = cv2.resize(prediction, (gt_w, gt_h), interpolation=cv2.INTER_NEAREST)\n",
    "            gray = np.uint8(prediction_resized)\n",
    "            color = colorize(gray, colors)\n",
    "\n",
    "            gray_path = os.path.join(gray_folder, img_name)\n",
    "            color_path = os.path.join(color_folder, img_name)\n",
    "\n",
    "            cv2.imwrite(gray_path, gray) # type: ignore\n",
    "            color.save(color_path)\n",
    "\n",
    "    return gray_folder, color_folder\n",
    "# -----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqnC6C_YkqRO"
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "# **Train the Network**\n",
    "\n",
    "## **Training & Testing Functions**\n",
    "\n",
    "Utility functions for handling model predictions and output management during evaluation. These functions create necessary directories, move prediction files from temporary folders to final result locations, and convert grayscale segmentation masks into colorised images using a predefined palette for easier visualisation. The main evaluation function runs inference on test images, saves both grayscale and colorised predictions, and prepares the outputs for accuracy calculations or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2117560,
     "status": "ok",
     "timestamp": 1747835847097,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "JvzZ3HxFkt_c",
    "outputId": "b893b385-5257-4cd4-83e1-612bf3f3acff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0 loss=2.1185545921325684\n",
      "epoch 0 iter 1 loss=1.992172122001648\n",
      "epoch 0 iter 2 loss=1.7288891077041626\n",
      "epoch 0 iter 3 loss=1.6026997566223145\n",
      "epoch 0 iter 4 loss=1.5068663358688354\n",
      "epoch 0 iter 5 loss=1.3739416599273682\n",
      "epoch 0 iter 6 loss=1.2956435680389404\n",
      "epoch 0 iter 7 loss=1.1463112831115723\n",
      "epoch 0 iter 8 loss=1.1935977935791016\n",
      "epoch 0 iter 9 loss=1.122610092163086\n",
      "epoch 0 iter 10 loss=0.9866684675216675\n",
      "epoch 0 iter 11 loss=0.9198613166809082\n",
      "epoch 0 iter 12 loss=0.8390737771987915\n",
      "epoch 0 iter 13 loss=0.8293736577033997\n",
      "epoch 0 iter 14 loss=0.8604687452316284\n",
      "epoch 0 iter 15 loss=0.8131034970283508\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Optimiser Step\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()                      \n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m iter \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m loss=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, \u001b[38;5;28miter\u001b[39m, \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Step the Learning Rate Scheduler\u001b[39;00m\n\u001b[1;32m     34\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ------------------------------------------ TRAINING LOOP ------------------------------------------\n",
    "mIoU = 0.0             # Track the Best Mean Intersection Over Union (mIoU) Score\n",
    "IoU_list = []          # Store mIoU for Each Evaluation Step\n",
    "train_loss = []        # Store Loss Values for Plotting or Analysis\n",
    "evl_each = True        # Flag to Control Whether Evaluation Happens After Each Epoch\n",
    "\n",
    "# Loop Through the Number of Epochs Defined Earlier\n",
    "for epoch in range(epochs):\n",
    "    # Set Model to Training Mode (Enables Dropout, Batchnorm, etc.)\n",
    "    segNet.train()  \n",
    "\n",
    "    # Iterate Over All Batches in the Training DataLoader\n",
    "    for iter, (imgs, labels) in enumerate(train_loader):\n",
    "        # Forward Pass\n",
    "        pred = segNet(imgs.to(device))                    \n",
    "        \n",
    "        # Reset Gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute Loss\n",
    "        loss = criterion(pred, labels.long().to(device))\n",
    "\n",
    "        # Store Loss\n",
    "        train_loss.append(loss.detach().cpu().numpy().item())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimiser Step\n",
    "        optimizer.step()                      \n",
    "        print('epoch {} iter {} loss={}'.format(epoch, iter, loss.data.cpu().numpy()))\n",
    "\n",
    "    # Step the Learning Rate Scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "# ------------------------------------------ EVALUATION ------------------------------------------\n",
    "    # Perform Evaluation if Enabled & Current Epoch > 100\n",
    "    if evl_each and epoch > 0:\n",
    "        # Set Model to Evaluation Mode (Disables Dropout, etc.)\n",
    "        segNet.eval()  \n",
    "\n",
    "        # Generate Predictions & Save Grayscale + Colourised Outputs\n",
    "        gray_folder, color_folder = get_predictions(segNet, dataFolder, device)\n",
    "        \n",
    "        # Switch Back to Training Mode for Next Epoch\n",
    "        segNet.train()  \n",
    "\n",
    "        # Calculate mIoU for this Epoch\n",
    "        temp_mIoU = cal_acc(gray_folder, os.path.join(dataFolder, 'testing/label'))\n",
    "        IoU_list.append(temp_mIoU)\n",
    "        # scheduler.step(temp_mIoU)  # For ReduceLROnPlateau\n",
    "\n",
    "        # Save Model if mIoU has Improved\n",
    "        if temp_mIoU > mIoU:\n",
    "            # Update Best mIoU Score\n",
    "            mIoU = temp_mIoU  \n",
    "\n",
    "            # Save Model\n",
    "            torch.save(segNet.state_dict(), './Models/OfficalModel.pth')  \n",
    "\n",
    "            # Move the Temporary Prediction Results to Final Output Folders\n",
    "            move_folders(gray_folder, color_folder, \n",
    "                         gray_folder.replace('temp_', ''), \n",
    "                         color_folder.replace('temp_', ''))\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Print The Final mIoU\n",
    "print('The final mIoU is : {:.4f}.'.format(mIoU)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## **Training Loss & IoU Performance**\n",
    "\n",
    "The first plot shows the training loss over iterations, illustrating how the model’s error decreases during training. The second plot tracks the Intersection over Union (IoU) metric from epoch 51 onward, reflecting improvements in segmentation accuracy as training progresses. Together, these visualisations help assess model learning and performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1747835847459,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "sM4LDbAO3OzL",
    "outputId": "b2f561f1-95e7-46c1-b983-23452360cab5"
   },
   "outputs": [],
   "source": [
    "# Make a Plot for the Data\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "\n",
    "# Add Title & Axis Labels\n",
    "plt.title(\"Training Loss Over Iterations\", fontweight = \"bold\", fontsize=\"16\")\n",
    "plt.xlabel(\"Training Iterations\", fontweight = \"bold\")\n",
    "plt.ylabel(\"Loss\", fontweight = \"bold\")\n",
    "\n",
    "# Add Grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Show Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1747835847518,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "1jBwoTvq3r7d",
    "outputId": "71632fe5-ac7e-46e8-df5b-d98ee21c3337"
   },
   "outputs": [],
   "source": [
    "# Epochs from 51 to 200\n",
    "epochs = list(range(1, 200))  \n",
    "\n",
    "# Plot IoU vs. Epoch\n",
    "plt.plot(epochs, IoU_list, label=\"IoU\")\n",
    "\n",
    "# Add Title & Axis Labels\n",
    "plt.title(\"IoU Over Epochs\", fontweight = \"bold\", fontsize=\"16\")\n",
    "plt.xlabel(\"Epoch\", fontweight = \"bold\")\n",
    "plt.ylabel(\"IoU\", fontweight = \"bold\")\n",
    "\n",
    "# Add Grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Show Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnJBjS-Hli40"
   },
   "source": [
    "<br/>\n",
    "\n",
    "## **Randomised Prediction Test Results from the Model:**\n",
    "\n",
    "This function randomly selects a few predicted segmentation masks and their corresponding original images, then displays them side-by-side. It helps qualitatively assess the model’s performance by providing a clear comparison between the input images and the predicted outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_predictions(temp_colour_folder, train_image_folder, num_images=3):\n",
    "    \"\"\"\n",
    "    Picks a few random predicted segmentation mask images and their corresponding original images,\n",
    "    then displays them side-by-side for visual comparison.\n",
    "    \n",
    "    Args:\n",
    "        temp_color_folder (str): Path to folder containing predicted color masks.\n",
    "        train_image_folder (str): Path to folder containing original training images.\n",
    "        num_images (int): Number of random samples to display.\n",
    "    \"\"\"\n",
    "\n",
    "    # List All Predicted Colour Mask Image Filenames\n",
    "    pred_images = os.listdir(temp_colour_folder)\n",
    "    \n",
    "    # Randomly Select 'num_images' Filenames From the Predictions List\n",
    "    selected_imgs = random.sample(pred_images, num_images)\n",
    "    \n",
    "    # Set Up Matplotlib Figure with Enough Height to Show All Selected Images Clearly\n",
    "    plt.figure(figsize=(15, num_images * 5))\n",
    "\n",
    "    # Loop Over the Selected Image Names & Plot Each Original & Predicted Pair\n",
    "    for i, img_name in enumerate(selected_imgs):\n",
    "        # Load Predicted Colour Mask Image (OpenCV Loads in BGR by Default)\n",
    "        pred_img = cv2.imread(os.path.join(temp_colour_folder, img_name))\n",
    "\n",
    "        # Convert Predicted Mask Image From BGR to RGB for Proper Colour Display in matplotlib\n",
    "        pred_img = cv2.cvtColor(pred_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load the Corresponding Original Training Image (also BGR)\n",
    "        orig_img = cv2.imread(os.path.join(train_image_folder, img_name))\n",
    "\n",
    "        # Convert Original Image from BGR to RGB for Display\n",
    "        orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Plot Original Image in Left Column\n",
    "        plt.subplot(num_images, 2, 2*i + 1)\n",
    "        plt.imshow(orig_img)\n",
    "        plt.title(f'Original Image: {img_name}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot Predicted Mask in Right Column\n",
    "        plt.subplot(num_images, 2, 2*i + 2)\n",
    "        plt.imshow(pred_img)\n",
    "        plt.title(f'Predicted Mask: {img_name}')\n",
    "        plt.axis('off') \n",
    "\n",
    "    # Adjust Spacing So Titles & Images Don't Overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the Plot Window\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Folder Paths for Predicted Masks & Original Images\n",
    "temp_color_folder = './TempFiles/color'\n",
    "train_image_folder = './seg_data/testing/image'\n",
    "\n",
    "# Call the Function to Display 3 Random Prediction Results\n",
    "show_random_predictions(temp_color_folder, train_image_folder, num_images=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FlshC7yryJZ"
   },
   "source": [
    "<br/><br>\n",
    "\n",
    "# **FLOPs**\n",
    "\n",
    "This section calculates the total number of floating point operations (FLOPs) the model performs during a single forward pass, providing a measure of its computational cost. Using the fvcore library, we quantify the model’s complexity in GFLOPs, which helps compare efficiency across different architectures.\n",
    "\n",
    "In deep learning, FLOPs *(Floating Point Operations)* quantify the total number of arithmetic operations, such as additions, multiplications, and divisions, that a model performs during a single forward pass *(i.e. when making a prediction)*. This metric serves as an indicator of a model’s computational complexity. When discussing large-scale models, FLOPs are often expressed in GFLOPs *(Giga Floating Point Operations)*, where 1 GFLOP equals one billion operations. This unit helps in comparing the computational demands of different models.\n",
    "\n",
    "**DO NOT MODIFY THIS CODE!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4944,
     "status": "ok",
     "timestamp": 1747835852463,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "C_D3Xsvu467x",
    "outputId": "df0bf371-a6e5-4b59-a25e-b2adf30e016f"
   },
   "outputs": [],
   "source": [
    "# We use Fvcore to Calculate the FLOPs\n",
    "!pip3 install fvcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3669,
     "status": "ok",
     "timestamp": 1747835856134,
     "user": {
      "displayName": "Jianqiao Zheng",
      "userId": "15895953406238580676"
     },
     "user_tz": -570
    },
    "id": "J5fUBat44-hT",
    "outputId": "3d21678e-e92b-4028-ba64-4177526ee771"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::mul encountered 28 time(s)\n",
      "Unsupported operator aten::softmax encountered 28 time(s)\n",
      "Unsupported operator aten::add encountered 56 time(s)\n",
      "Unsupported operator aten::gelu encountered 28 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.block1.1.drop_path, encoder.block1.2.drop_path, encoder.block2.0.drop_path, encoder.block2.1.drop_path, encoder.block2.2.drop_path, encoder.block2.3.drop_path, encoder.block3.0.drop_path, encoder.block3.1.drop_path, encoder.block3.10.drop_path, encoder.block3.11.drop_path, encoder.block3.12.drop_path, encoder.block3.13.drop_path, encoder.block3.14.drop_path, encoder.block3.15.drop_path, encoder.block3.16.drop_path, encoder.block3.17.drop_path, encoder.block3.2.drop_path, encoder.block3.3.drop_path, encoder.block3.4.drop_path, encoder.block3.5.drop_path, encoder.block3.6.drop_path, encoder.block3.7.drop_path, encoder.block3.8.drop_path, encoder.block3.9.drop_path, encoder.block4.0.drop_path, encoder.block4.1.drop_path, encoder.block4.2.drop_path, segmentation_head.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 84.03 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "# Modifying the Size (3, 375, 1242) is ***NOT*** Allowed\n",
    "input = torch.randn(1, 3, 375, 1242)\n",
    "\n",
    "# Get the Network & its FLOPs\n",
    "model = smp.Segformer(encoder_name=\"mit_b3\", encoder_weights=\"imagenet\", in_channels=3, classes=19).to(device).cpu()\n",
    "model.eval()\n",
    "input = input.cpu()\n",
    "flops = FlopCountAnalysis(model, input)\n",
    "print(f\"FLOPs: {flops.total()/1e9:.2f} GFLOPs\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
